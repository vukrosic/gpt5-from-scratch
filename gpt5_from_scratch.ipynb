{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code & Train GPT-5 From Scratch - Step by Step\n",
        "\n",
        "> **To train the model just run the cells below!**\n",
        "\n",
        "---\n",
        "\n",
        "GPT-5 is not open sourced, however, we can make an educated guess on how it's built:\n",
        "- **GPT architecture + latest advancements in LLM pretraining**\n",
        "\n",
        "If you need a reminder on GPT or base LLM architecture check:\n",
        "\n",
        "---\n",
        "\n",
        "🎓 **[🦙 LLaMA 4 From Scratch (first 2h 30min)](https://youtu.be/wcDV3l4CD14)**\n",
        "\n",
        "> In the first **2h 30min**, I give a **clear and intuitive explanation** of both:\n",
        "\n",
        "* 🧠 Attention Mechanism\n",
        "* 🧩 Tokens & Tokenizers\n",
        "\n",
        "Highly recommended if you're just starting out or want a solid refresher.\n",
        "\n",
        "---\n",
        "\n",
        "🎥 **[📘 GPT From Scratch by Andrej Karpathy](https://youtu.be/kCc8FmEb1nY)**\n",
        "\n",
        "> A legendary tutorial from Karpathy that walks through building a GPT model from scratch. Great for understanding the fundamentals!\n",
        "- If Andrej's course is too complex or difficult, then return to it after watching few more of the videos / courses below that you find more digestible.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Modern GPT architecture with latest advancements\n",
        "\n",
        "⚙️ For attention mechanism there are 3 options:\n",
        "1. Option 1: OpenAI invented its own attention mechanism, in this case it is almost certainly not too different from others, unless they compretely surpassed transformer architecture, which is unlikely to happen until 2026 or 2027. So their architecture is likely very similar to option 2 - GQA.\n",
        "\n",
        "2. Option 2: Grouped-Query Attention (GQA) - memory and compute efficienct attention\n",
        "- Explained in my course [Code & Train Qwen 3 From Scratch - Full Course](https://youtu.be/wM-KP_wNAeY) starting at 16:35\n",
        "\n",
        "3. Option 3 (less likely): Multihead Latent Attention (MLA)\n",
        "- Given that some of the latest LLMs are using Multihead Latent Attention (MLA) by DeepSeek, there is some chance GPT-5 also uses it instead of the GQA - you can learn how to code it in my [Code DeepSeek From Scratch Course](https://youtu.be/TfEG0TwueTs)\n",
        "- Advantage of MLA over GQA is lower memory usage, however it's a bit more complex to build.\n",
        "- I recommend learning all of these as it will help you understand neural networks, transformers and how to surpass transformers and invent the next architecture.\n",
        "\n",
        "💡 Rotary Positional Embeddings (RoPE) for better performance and context window extrapolation\n",
        "- 📌 **[Rotary Positional Embeddings & Rotation Matrix + Python LLM Code](https://youtu.be/wiJ-OU-URYg)**\n",
        "- 🧠 **[Get SMARTER Than 99% of AI Researchers](https://youtu.be/X0JryI85hL0)** - Beginning part\n",
        "- 🛠️ **[RoPE In DeepSeek V3 – Code Step by Step](https://youtu.be/Rs9tLDSMUkM)**\n",
        "- 🏋️ **[Excercises with ChatGPT Chat](https://chatgpt.com/share/68945a01-8d48-8002-8cf0-04b7f6db744b)**\n",
        "\n",
        "🚀 Muon optimizer using Newton-Schulz orthogonalization for better weight updates, faster learning with less data\n",
        "- This is the new best optimizer for 2D matrices, while AdamW is used for other parts of LLM. Highly likely Muon is used in GPT-5 as [OpenAI's researcher is tied to its invention](https://kellerjordan.github.io/posts/muon/).\n",
        "- 🔁 [Backpropagation From Scratch](https://youtu.be/W8g1hvW4Wic) — Understand gradients deeply\n",
        "- 🧠 [Orthonormal Matrix Intuition](https://youtu.be/FbYRZpBgFz4) — Key concept behind Muon’s update step\n",
        "- Search for \"Muon\" on YouTube, you will find more tutorials.\n",
        "\n",
        "> For all other things (below and above), watch my [Code & Train Qwen 3 From Scratch - Full Course](https://youtu.be/wM-KP_wNAeY) - I built and trained it on modern GPT architecture with latest advancements\n",
        "\n",
        "📐 QK-Norm with RMSNorm for improved numerical / training stability\n",
        "\n",
        "🔁 Hybrid optimization using Muon for matrices and AdamW for other parameters\n",
        "\n",
        "🔄 SwiGLU activation and deep residual learning in the feedforward layers\n",
        "\n",
        "🔢 Efficient dataset tokenization and caching with HuggingFace Datasets and Transformers\n",
        "\n",
        "🧪 Validation metrics including loss, accuracy, and perplexity\n",
        "\n",
        "🧵 Gradient accumulation + AMP (Automatic Mixed Precision) training for larger batch sizes\n",
        "\n",
        "🎛️ Cosine learning rate scheduling with warmup\n",
        "\n",
        "Find more tutorials / courses on AI research and engineering on [my YouTube](https://www.youtube.com/channel/UC7XJj9pv_11a11FUxCMz15g).\n",
        "\n"
      ],
      "metadata": {
        "id": "ihfuumemPZUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "5Ae2EEMtPbq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`max_steps: int = 2000` determines the duration of training, 2000 steps is about 12 minutes"
      ],
      "metadata": {
        "id": "2jkWItbTPnQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If it asks you for `HF_TOKEN`, you can just click `cancel` as it doesn't need it."
      ],
      "metadata": {
        "id": "CuXUFdg3QHfb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwDqe9Y6LcNl",
        "outputId": "5240a3d2-5da2-4726-9bc9-006819c23407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Device: CUDA\n",
            "GPU: Tesla T4\n",
            "Memory: 15.8 GB\n",
            "🌱 Set all seeds to 42\n",
            "\n",
            "📋 Model Configuration:\n",
            "   Architecture: 384d, 6L, 8H, 1536ff\n",
            "   Training: 2000 steps, batch size 24\n",
            "   Data: 500,000 tokens, seq_len 512\n",
            "📦 Loading cached data from data_cache/tokenized_data_2000_500000.pkl\n",
            "✅ Loaded 2000 documents, 500,000 tokens from cache\n",
            "📊 Dataset: 449540 train, 49948 val samples\n",
            "\n",
            "🚀 Training Small model with Muon optimizer\n",
            "🌱 Set all seeds to 42\n",
            "  📊 Total parameters: 32,150,976\n",
            "  Muon parameters: 13,271,040\n",
            "  AdamW parameters: 18,879,936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Training:   0%|          | 0/2000 [00:00<?, ?it/s]\u001b[A\n",
            "Training:   0%|          | 0/2000 [00:00<?, ?it/s, loss=10.8028, acc=0.015, ppl=49156.2, lr=0.00e+00]\u001b[A\n",
            "Training:   0%|          | 10/2000 [00:04<14:15,  2.33it/s, loss=10.8028, acc=0.015, ppl=49156.2, lr=0.00e+00]\u001b[A\n",
            "Training:   0%|          | 10/2000 [00:04<14:15,  2.33it/s, loss=10.8007, acc=0.014, ppl=49055.0, lr=2.00e-04]\u001b[A\n",
            "Training:   1%|          | 20/2000 [00:08<13:18,  2.48it/s, loss=10.8007, acc=0.014, ppl=49055.0, lr=2.00e-04]\u001b[A\n",
            "Training:   1%|          | 20/2000 [00:08<13:18,  2.48it/s, loss=10.7808, acc=0.016, ppl=48088.1, lr=5.00e-04]\u001b[A\n",
            "Training:   2%|▏         | 30/2000 [00:11<12:11,  2.69it/s, loss=10.7808, acc=0.016, ppl=48088.1, lr=5.00e-04]\u001b[A\n",
            "Training:   2%|▏         | 30/2000 [00:12<12:11,  2.69it/s, loss=10.7480, acc=0.016, ppl=46535.3, lr=7.00e-04]\u001b[A\n",
            "Training:   2%|▏         | 40/2000 [00:15<12:15,  2.66it/s, loss=10.7480, acc=0.016, ppl=46535.3, lr=7.00e-04]\u001b[A\n",
            "Training:   2%|▏         | 40/2000 [00:15<12:15,  2.66it/s, loss=10.7048, acc=0.016, ppl=44568.8, lr=1.00e-03]\u001b[A\n",
            "Training:   2%|▎         | 50/2000 [00:18<11:39,  2.79it/s, loss=10.7048, acc=0.016, ppl=44568.8, lr=1.00e-03]\u001b[A\n",
            "Training:   2%|▎         | 50/2000 [00:19<11:39,  2.79it/s, loss=10.6530, acc=0.016, ppl=42319.2, lr=1.20e-03]\u001b[A\n",
            "Training:   3%|▎         | 60/2000 [00:22<11:47,  2.74it/s, loss=10.6530, acc=0.016, ppl=42319.2, lr=1.20e-03]\u001b[A\n",
            "Training:   3%|▎         | 60/2000 [00:22<11:47,  2.74it/s, loss=10.5811, acc=0.015, ppl=39382.9, lr=1.50e-03]\u001b[A\n",
            "Training:   4%|▎         | 70/2000 [00:25<11:21,  2.83it/s, loss=10.5811, acc=0.015, ppl=39382.9, lr=1.50e-03]\u001b[A\n",
            "Training:   4%|▎         | 70/2000 [00:26<11:21,  2.83it/s, loss=10.4804, acc=0.015, ppl=35610.1, lr=1.70e-03]\u001b[A\n",
            "Training:   4%|▍         | 80/2000 [00:29<11:32,  2.77it/s, loss=10.4804, acc=0.015, ppl=35610.1, lr=1.70e-03]\u001b[A\n",
            "Training:   4%|▍         | 80/2000 [00:29<11:32,  2.77it/s, loss=10.3496, acc=0.015, ppl=31244.9, lr=2.00e-03]\u001b[A\n",
            "Training:   4%|▍         | 90/2000 [00:32<11:07,  2.86it/s, loss=10.3496, acc=0.015, ppl=31244.9, lr=2.00e-03]\u001b[A\n",
            "Training:   4%|▍         | 90/2000 [00:33<11:07,  2.86it/s, loss=10.1831, acc=0.015, ppl=26452.3, lr=2.20e-03]\u001b[A\n",
            "Training:   5%|▌         | 100/2000 [00:36<11:19,  2.80it/s, loss=10.1831, acc=0.015, ppl=26452.3, lr=2.20e-03]\u001b[A\n",
            "Training:   5%|▌         | 100/2000 [00:36<11:19,  2.80it/s, loss=9.9458, acc=0.014, ppl=20865.1, lr=2.50e-03] \u001b[A\n",
            "Training:   6%|▌         | 110/2000 [00:39<10:58,  2.87it/s, loss=9.9458, acc=0.014, ppl=20865.1, lr=2.50e-03]\u001b[A\n",
            "Training:   6%|▌         | 110/2000 [00:40<10:58,  2.87it/s, loss=9.7371, acc=0.015, ppl=16934.1, lr=2.70e-03]\u001b[A\n",
            "Training:   6%|▌         | 120/2000 [00:43<11:10,  2.80it/s, loss=9.7371, acc=0.015, ppl=16934.1, lr=2.70e-03]\u001b[A\n",
            "Training:   6%|▌         | 120/2000 [00:43<11:10,  2.80it/s, loss=9.3791, acc=0.015, ppl=11838.2, lr=3.00e-03]\u001b[A\n",
            "Training:   6%|▋         | 130/2000 [00:46<10:49,  2.88it/s, loss=9.3791, acc=0.015, ppl=11838.2, lr=3.00e-03]\u001b[A\n",
            "Training:   6%|▋         | 130/2000 [00:47<10:49,  2.88it/s, loss=9.1484, acc=0.017, ppl=9399.6, lr=3.20e-03] \u001b[A\n",
            "Training:   7%|▋         | 140/2000 [00:50<11:02,  2.81it/s, loss=9.1484, acc=0.017, ppl=9399.6, lr=3.20e-03]\u001b[A\n",
            "Training:   7%|▋         | 140/2000 [00:50<11:02,  2.81it/s, loss=8.7885, acc=0.022, ppl=6558.5, lr=3.50e-03]\u001b[A\n",
            "Training:   8%|▊         | 150/2000 [00:53<10:44,  2.87it/s, loss=8.7885, acc=0.022, ppl=6558.5, lr=3.50e-03]\u001b[A\n",
            "Training:   8%|▊         | 150/2000 [00:54<10:44,  2.87it/s, loss=8.5138, acc=0.039, ppl=4983.1, lr=3.70e-03]\u001b[A\n",
            "Training:   8%|▊         | 160/2000 [00:57<10:55,  2.81it/s, loss=8.5138, acc=0.039, ppl=4983.1, lr=3.70e-03]\u001b[A\n",
            "Training:   8%|▊         | 160/2000 [00:57<10:55,  2.81it/s, loss=8.1704, acc=0.072, ppl=3534.6, lr=4.00e-03]\u001b[A\n",
            "Training:   8%|▊         | 170/2000 [01:00<10:35,  2.88it/s, loss=8.1704, acc=0.072, ppl=3534.6, lr=4.00e-03]\u001b[A\n",
            "Training:   8%|▊         | 170/2000 [01:01<10:35,  2.88it/s, loss=7.9453, acc=0.086, ppl=2822.2, lr=4.20e-03]\u001b[A\n",
            "Training:   9%|▉         | 180/2000 [01:04<10:49,  2.80it/s, loss=7.9453, acc=0.086, ppl=2822.2, lr=4.20e-03]\u001b[A\n",
            "Training:   9%|▉         | 180/2000 [01:04<10:49,  2.80it/s, loss=7.7007, acc=0.079, ppl=2209.8, lr=4.50e-03]\u001b[A\n",
            "Training:  10%|▉         | 190/2000 [01:08<10:39,  2.83it/s, loss=7.7007, acc=0.079, ppl=2209.8, lr=4.50e-03]\u001b[A\n",
            "Training:  10%|▉         | 190/2000 [01:08<10:39,  2.83it/s, loss=7.5115, acc=0.085, ppl=1829.0, lr=4.70e-03]\u001b[A\n",
            "Training:  10%|█         | 200/2000 [01:11<10:54,  2.75it/s, loss=7.5115, acc=0.085, ppl=1829.0, lr=4.70e-03]\u001b[A\n",
            "Training:  10%|█         | 200/2000 [01:12<10:54,  2.75it/s, loss=7.3087, acc=0.087, ppl=1493.3, lr=5.00e-03]\u001b[A\n",
            "Training:  10%|█         | 210/2000 [01:15<10:33,  2.83it/s, loss=7.3087, acc=0.087, ppl=1493.3, lr=5.00e-03]\u001b[A\n",
            "Training:  10%|█         | 210/2000 [01:15<10:33,  2.83it/s, loss=7.2170, acc=0.101, ppl=1362.5, lr=5.20e-03]\u001b[A\n",
            "Training:  11%|█         | 220/2000 [01:19<10:48,  2.74it/s, loss=7.2170, acc=0.101, ppl=1362.5, lr=5.20e-03]\u001b[A\n",
            "Training:  11%|█         | 220/2000 [01:19<10:48,  2.74it/s, loss=7.1611, acc=0.114, ppl=1288.4, lr=5.50e-03]\u001b[A\n",
            "Training:  12%|█▏        | 230/2000 [01:22<10:31,  2.80it/s, loss=7.1611, acc=0.114, ppl=1288.4, lr=5.50e-03]\u001b[A\n",
            "Training:  12%|█▏        | 230/2000 [01:23<10:31,  2.80it/s, loss=7.1061, acc=0.117, ppl=1219.4, lr=5.70e-03]\u001b[A\n",
            "Training:  12%|█▏        | 240/2000 [01:26<10:40,  2.75it/s, loss=7.1061, acc=0.117, ppl=1219.4, lr=5.70e-03]\u001b[A\n",
            "Training:  12%|█▏        | 240/2000 [01:26<10:40,  2.75it/s, loss=7.0065, acc=0.129, ppl=1103.8, lr=6.00e-03]\u001b[A\n",
            "Training:  12%|█▎        | 250/2000 [01:29<10:20,  2.82it/s, loss=7.0065, acc=0.129, ppl=1103.8, lr=6.00e-03]\u001b[A\n",
            "Training:  12%|█▎        | 250/2000 [01:30<10:20,  2.82it/s, loss=7.0200, acc=0.135, ppl=1118.7, lr=6.20e-03]\u001b[A\n",
            "Training:  13%|█▎        | 260/2000 [01:33<10:29,  2.76it/s, loss=7.0200, acc=0.135, ppl=1118.7, lr=6.20e-03]\u001b[A\n",
            "Training:  13%|█▎        | 260/2000 [01:33<10:29,  2.76it/s, loss=6.8508, acc=0.141, ppl=944.7, lr=6.50e-03] \u001b[A\n",
            "Training:  14%|█▎        | 270/2000 [01:36<10:08,  2.84it/s, loss=6.8508, acc=0.141, ppl=944.7, lr=6.50e-03]\u001b[A\n",
            "Training:  14%|█▎        | 270/2000 [01:37<10:08,  2.84it/s, loss=6.7639, acc=0.150, ppl=866.0, lr=6.70e-03]\u001b[A\n",
            "Training:  14%|█▍        | 280/2000 [01:40<10:17,  2.78it/s, loss=6.7639, acc=0.150, ppl=866.0, lr=6.70e-03]\u001b[A\n",
            "Training:  14%|█▍        | 280/2000 [01:40<10:17,  2.78it/s, loss=6.7645, acc=0.152, ppl=866.5, lr=7.00e-03]\u001b[A\n",
            "Training:  14%|█▍        | 290/2000 [01:43<09:57,  2.86it/s, loss=6.7645, acc=0.152, ppl=866.5, lr=7.00e-03]\u001b[A\n",
            "Training:  14%|█▍        | 290/2000 [01:44<09:57,  2.86it/s, loss=6.5612, acc=0.154, ppl=707.1, lr=7.20e-03]\u001b[A\n",
            "Training:  15%|█▌        | 300/2000 [01:47<10:08,  2.79it/s, loss=6.5612, acc=0.154, ppl=707.1, lr=7.20e-03]\u001b[A\n",
            "Training:  15%|█▌        | 300/2000 [01:47<10:08,  2.79it/s, loss=6.3311, acc=0.170, ppl=561.8, lr=7.50e-03]\u001b[A\n",
            "Training:  16%|█▌        | 310/2000 [01:50<09:50,  2.86it/s, loss=6.3311, acc=0.170, ppl=561.8, lr=7.50e-03]\u001b[A\n",
            "Training:  16%|█▌        | 310/2000 [01:51<09:50,  2.86it/s, loss=6.4285, acc=0.175, ppl=619.2, lr=7.70e-03]\u001b[A\n",
            "Training:  16%|█▌        | 320/2000 [01:54<09:59,  2.80it/s, loss=6.4285, acc=0.175, ppl=619.2, lr=7.70e-03]\u001b[A\n",
            "Training:  16%|█▌        | 320/2000 [01:54<09:59,  2.80it/s, loss=6.3874, acc=0.175, ppl=594.3, lr=8.00e-03]\u001b[A\n",
            "Training:  16%|█▋        | 330/2000 [01:57<09:41,  2.87it/s, loss=6.3874, acc=0.175, ppl=594.3, lr=8.00e-03]\u001b[A\n",
            "Training:  16%|█▋        | 330/2000 [01:58<09:41,  2.87it/s, loss=6.1572, acc=0.171, ppl=472.1, lr=8.20e-03]\u001b[A\n",
            "Training:  17%|█▋        | 340/2000 [02:01<09:51,  2.81it/s, loss=6.1572, acc=0.171, ppl=472.1, lr=8.20e-03]\u001b[A\n",
            "Training:  17%|█▋        | 340/2000 [02:01<09:51,  2.81it/s, loss=6.0801, acc=0.182, ppl=437.1, lr=8.50e-03]\u001b[A\n",
            "Training:  18%|█▊        | 350/2000 [02:04<09:33,  2.88it/s, loss=6.0801, acc=0.182, ppl=437.1, lr=8.50e-03]\u001b[A\n",
            "Training:  18%|█▊        | 350/2000 [02:05<09:33,  2.88it/s, loss=5.9841, acc=0.183, ppl=397.1, lr=8.70e-03]\u001b[A\n",
            "Training:  18%|█▊        | 360/2000 [02:08<09:42,  2.81it/s, loss=5.9841, acc=0.183, ppl=397.1, lr=8.70e-03]\u001b[A\n",
            "Training:  18%|█▊        | 360/2000 [02:08<09:42,  2.81it/s, loss=5.9918, acc=0.192, ppl=400.1, lr=9.00e-03]\u001b[A\n",
            "Training:  18%|█▊        | 370/2000 [02:11<09:24,  2.89it/s, loss=5.9918, acc=0.192, ppl=400.1, lr=9.00e-03]\u001b[A\n",
            "Training:  18%|█▊        | 370/2000 [02:12<09:24,  2.89it/s, loss=5.8166, acc=0.192, ppl=335.8, lr=9.20e-03]\u001b[A\n",
            "Training:  19%|█▉        | 380/2000 [02:15<09:34,  2.82it/s, loss=5.8166, acc=0.192, ppl=335.8, lr=9.20e-03]\u001b[A\n",
            "Training:  19%|█▉        | 380/2000 [02:15<09:34,  2.82it/s, loss=5.6587, acc=0.200, ppl=286.8, lr=9.50e-03]\u001b[A\n",
            "Training:  20%|█▉        | 390/2000 [02:18<09:17,  2.89it/s, loss=5.6587, acc=0.200, ppl=286.8, lr=9.50e-03]\u001b[A\n",
            "Training:  20%|█▉        | 390/2000 [02:19<09:17,  2.89it/s, loss=5.6870, acc=0.201, ppl=295.0, lr=9.70e-03]\u001b[A\n",
            "Training:  20%|██        | 400/2000 [02:22<09:26,  2.82it/s, loss=5.6870, acc=0.201, ppl=295.0, lr=9.70e-03]\u001b[A\n",
            "Training:  20%|██        | 400/2000 [02:22<09:26,  2.82it/s, loss=5.6463, acc=0.200, ppl=283.3, lr=1.00e-02]\u001b[A\n",
            "Training:  20%|██        | 410/2000 [02:25<09:09,  2.89it/s, loss=5.6463, acc=0.200, ppl=283.3, lr=1.00e-02]\u001b[A\n",
            "Training:  20%|██        | 410/2000 [02:26<09:09,  2.89it/s, loss=5.6009, acc=0.205, ppl=270.7, lr=1.00e-02]\u001b[A\n",
            "Training:  21%|██        | 420/2000 [02:29<09:20,  2.82it/s, loss=5.6009, acc=0.205, ppl=270.7, lr=1.00e-02]\u001b[A\n",
            "Training:  21%|██        | 420/2000 [02:29<09:20,  2.82it/s, loss=5.6109, acc=0.205, ppl=273.4, lr=1.00e-02]\u001b[A\n",
            "Training:  22%|██▏       | 430/2000 [02:32<09:03,  2.89it/s, loss=5.6109, acc=0.205, ppl=273.4, lr=1.00e-02]\u001b[A\n",
            "Training:  22%|██▏       | 430/2000 [02:33<09:03,  2.89it/s, loss=5.5572, acc=0.207, ppl=259.1, lr=1.00e-02]\u001b[A\n",
            "Training:  22%|██▏       | 440/2000 [02:36<09:14,  2.82it/s, loss=5.5572, acc=0.207, ppl=259.1, lr=1.00e-02]\u001b[A\n",
            "Training:  22%|██▏       | 440/2000 [02:36<09:14,  2.82it/s, loss=5.4647, acc=0.206, ppl=236.2, lr=1.00e-02]\u001b[A\n",
            "Training:  22%|██▎       | 450/2000 [02:39<08:57,  2.89it/s, loss=5.4647, acc=0.206, ppl=236.2, lr=1.00e-02]\u001b[A\n",
            "Training:  22%|██▎       | 450/2000 [02:40<08:57,  2.89it/s, loss=5.3177, acc=0.211, ppl=203.9, lr=1.00e-02]\u001b[A\n",
            "Training:  23%|██▎       | 460/2000 [02:43<09:07,  2.81it/s, loss=5.3177, acc=0.211, ppl=203.9, lr=1.00e-02]\u001b[A\n",
            "Training:  23%|██▎       | 460/2000 [02:43<09:07,  2.81it/s, loss=5.3506, acc=0.212, ppl=210.7, lr=1.00e-02]\u001b[A\n",
            "Training:  24%|██▎       | 470/2000 [02:46<08:50,  2.88it/s, loss=5.3506, acc=0.212, ppl=210.7, lr=1.00e-02]\u001b[A\n",
            "Training:  24%|██▎       | 470/2000 [02:47<08:50,  2.88it/s, loss=5.1576, acc=0.222, ppl=173.7, lr=1.00e-02]\u001b[A\n",
            "Training:  24%|██▍       | 480/2000 [02:50<08:59,  2.81it/s, loss=5.1576, acc=0.222, ppl=173.7, lr=1.00e-02]\u001b[A\n",
            "Training:  24%|██▍       | 480/2000 [02:50<08:59,  2.81it/s, loss=5.1176, acc=0.226, ppl=166.9, lr=1.00e-02]\u001b[A\n",
            "Training:  24%|██▍       | 490/2000 [02:53<08:44,  2.88it/s, loss=5.1176, acc=0.226, ppl=166.9, lr=1.00e-02]\u001b[A\n",
            "Training:  24%|██▍       | 490/2000 [02:54<08:44,  2.88it/s, loss=5.0815, acc=0.241, ppl=161.0, lr=1.00e-02]\u001b[A\n",
            "Training:  25%|██▌       | 500/2000 [02:57<08:53,  2.81it/s, loss=5.0815, acc=0.241, ppl=161.0, lr=1.00e-02]\u001b[A\n",
            "Training:  25%|██▌       | 500/2000 [02:58<08:53,  2.81it/s, loss=5.2135, acc=0.221, ppl=183.7, lr=1.00e-02]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 500: Val Loss: 4.9941, Val Acc: 0.2329, Val PPL: 147.55\n",
            "💾 Saved best model with val_loss: 4.9941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Training:  26%|██▌       | 510/2000 [03:13<18:02,  1.38it/s, loss=5.2135, acc=0.221, ppl=183.7, lr=1.00e-02]\u001b[A\n",
            "Training:  26%|██▌       | 510/2000 [03:14<18:02,  1.38it/s, loss=5.1443, acc=0.217, ppl=171.5, lr=1.00e-02]\u001b[A\n",
            "Training:  26%|██▌       | 520/2000 [03:17<15:38,  1.58it/s, loss=5.1443, acc=0.217, ppl=171.5, lr=1.00e-02]\u001b[A\n",
            "Training:  26%|██▌       | 520/2000 [03:18<15:38,  1.58it/s, loss=4.9569, acc=0.231, ppl=142.2, lr=9.99e-03]\u001b[A\n",
            "Training:  26%|██▋       | 530/2000 [03:21<13:39,  1.79it/s, loss=4.9569, acc=0.231, ppl=142.2, lr=9.99e-03]\u001b[A\n",
            "Training:  26%|██▋       | 530/2000 [03:22<13:39,  1.79it/s, loss=4.9854, acc=0.238, ppl=146.3, lr=9.99e-03]\u001b[A\n",
            "Training:  27%|██▋       | 540/2000 [03:25<12:14,  1.99it/s, loss=4.9854, acc=0.238, ppl=146.3, lr=9.99e-03]\u001b[A\n",
            "Training:  27%|██▋       | 540/2000 [03:25<12:14,  1.99it/s, loss=4.9169, acc=0.243, ppl=136.6, lr=9.99e-03]\u001b[A\n",
            "Training:  28%|██▊       | 550/2000 [03:28<10:52,  2.22it/s, loss=4.9169, acc=0.243, ppl=136.6, lr=9.99e-03]\u001b[A\n",
            "Training:  28%|██▊       | 550/2000 [03:29<10:52,  2.22it/s, loss=4.8744, acc=0.235, ppl=130.9, lr=9.99e-03]\u001b[A\n",
            "Training:  28%|██▊       | 560/2000 [03:32<10:15,  2.34it/s, loss=4.8744, acc=0.235, ppl=130.9, lr=9.99e-03]\u001b[A\n",
            "Training:  28%|██▊       | 560/2000 [03:32<10:15,  2.34it/s, loss=4.8438, acc=0.236, ppl=127.0, lr=9.99e-03]\u001b[A\n",
            "Training:  28%|██▊       | 570/2000 [03:35<09:28,  2.52it/s, loss=4.8438, acc=0.236, ppl=127.0, lr=9.99e-03]\u001b[A\n",
            "Training:  28%|██▊       | 570/2000 [03:36<09:28,  2.52it/s, loss=4.8594, acc=0.228, ppl=128.9, lr=9.99e-03]\u001b[A\n",
            "Training:  29%|██▉       | 580/2000 [03:39<09:14,  2.56it/s, loss=4.8594, acc=0.228, ppl=128.9, lr=9.99e-03]\u001b[A\n",
            "Training:  29%|██▉       | 580/2000 [03:39<09:14,  2.56it/s, loss=4.7714, acc=0.237, ppl=118.1, lr=9.99e-03]\u001b[A\n",
            "Training:  30%|██▉       | 590/2000 [03:42<08:43,  2.69it/s, loss=4.7714, acc=0.237, ppl=118.1, lr=9.99e-03]\u001b[A\n",
            "Training:  30%|██▉       | 590/2000 [03:43<08:43,  2.69it/s, loss=4.8172, acc=0.241, ppl=123.6, lr=9.99e-03]\u001b[A\n",
            "Training:  30%|███       | 600/2000 [03:46<08:41,  2.68it/s, loss=4.8172, acc=0.241, ppl=123.6, lr=9.99e-03]\u001b[A\n",
            "Training:  30%|███       | 600/2000 [03:46<08:41,  2.68it/s, loss=4.7483, acc=0.253, ppl=115.4, lr=9.98e-03]\u001b[A\n",
            "Training:  30%|███       | 610/2000 [03:49<08:18,  2.79it/s, loss=4.7483, acc=0.253, ppl=115.4, lr=9.98e-03]\u001b[A\n",
            "Training:  30%|███       | 610/2000 [03:50<08:18,  2.79it/s, loss=4.6862, acc=0.251, ppl=108.4, lr=9.98e-03]\u001b[A\n",
            "Training:  31%|███       | 620/2000 [03:53<08:21,  2.75it/s, loss=4.6862, acc=0.251, ppl=108.4, lr=9.98e-03]\u001b[A\n",
            "Training:  31%|███       | 620/2000 [03:53<08:21,  2.75it/s, loss=4.7513, acc=0.249, ppl=115.7, lr=9.98e-03]\u001b[A\n",
            "Training:  32%|███▏      | 630/2000 [03:56<08:02,  2.84it/s, loss=4.7513, acc=0.249, ppl=115.7, lr=9.98e-03]\u001b[A\n",
            "Training:  32%|███▏      | 630/2000 [03:57<08:02,  2.84it/s, loss=4.7481, acc=0.240, ppl=115.4, lr=9.98e-03]\u001b[A\n",
            "Training:  32%|███▏      | 640/2000 [04:00<08:08,  2.79it/s, loss=4.7481, acc=0.240, ppl=115.4, lr=9.98e-03]\u001b[A\n",
            "Training:  32%|███▏      | 640/2000 [04:00<08:08,  2.79it/s, loss=4.6185, acc=0.249, ppl=101.3, lr=9.98e-03]\u001b[A\n",
            "Training:  32%|███▎      | 650/2000 [04:03<07:51,  2.86it/s, loss=4.6185, acc=0.249, ppl=101.3, lr=9.98e-03]\u001b[A\n",
            "Training:  32%|███▎      | 650/2000 [04:04<07:51,  2.86it/s, loss=4.6697, acc=0.245, ppl=106.7, lr=9.98e-03]\u001b[A\n",
            "Training:  33%|███▎      | 660/2000 [04:07<07:58,  2.80it/s, loss=4.6697, acc=0.245, ppl=106.7, lr=9.98e-03]\u001b[A\n",
            "Training:  33%|███▎      | 660/2000 [04:07<07:58,  2.80it/s, loss=4.6467, acc=0.257, ppl=104.2, lr=9.97e-03]\u001b[A\n",
            "Training:  34%|███▎      | 670/2000 [04:10<07:42,  2.87it/s, loss=4.6467, acc=0.257, ppl=104.2, lr=9.97e-03]\u001b[A\n",
            "Training:  34%|███▎      | 670/2000 [04:11<07:42,  2.87it/s, loss=4.4939, acc=0.265, ppl=89.5, lr=9.97e-03] \u001b[A\n",
            "Training:  34%|███▍      | 680/2000 [04:14<07:49,  2.81it/s, loss=4.4939, acc=0.265, ppl=89.5, lr=9.97e-03]\u001b[A\n",
            "Training:  34%|███▍      | 680/2000 [04:14<07:49,  2.81it/s, loss=4.4324, acc=0.262, ppl=84.1, lr=9.97e-03]\u001b[A\n",
            "Training:  34%|███▍      | 690/2000 [04:17<07:34,  2.88it/s, loss=4.4324, acc=0.262, ppl=84.1, lr=9.97e-03]\u001b[A\n",
            "Training:  34%|███▍      | 690/2000 [04:18<07:34,  2.88it/s, loss=4.5177, acc=0.260, ppl=91.6, lr=9.97e-03]\u001b[A\n",
            "Training:  35%|███▌      | 700/2000 [04:21<07:41,  2.82it/s, loss=4.5177, acc=0.260, ppl=91.6, lr=9.97e-03]\u001b[A\n",
            "Training:  35%|███▌      | 700/2000 [04:21<07:41,  2.82it/s, loss=4.3477, acc=0.274, ppl=77.3, lr=9.97e-03]\u001b[A\n",
            "Training:  36%|███▌      | 710/2000 [04:24<07:27,  2.88it/s, loss=4.3477, acc=0.274, ppl=77.3, lr=9.97e-03]\u001b[A\n",
            "Training:  36%|███▌      | 710/2000 [04:25<07:27,  2.88it/s, loss=4.4319, acc=0.274, ppl=84.1, lr=9.96e-03]\u001b[A\n",
            "Training:  36%|███▌      | 720/2000 [04:28<07:34,  2.82it/s, loss=4.4319, acc=0.274, ppl=84.1, lr=9.96e-03]\u001b[A\n",
            "Training:  36%|███▌      | 720/2000 [04:28<07:34,  2.82it/s, loss=4.2559, acc=0.279, ppl=70.5, lr=9.96e-03]\u001b[A\n",
            "Training:  36%|███▋      | 730/2000 [04:31<07:20,  2.89it/s, loss=4.2559, acc=0.279, ppl=70.5, lr=9.96e-03]\u001b[A\n",
            "Training:  36%|███▋      | 730/2000 [04:32<07:20,  2.89it/s, loss=4.4662, acc=0.258, ppl=87.0, lr=9.96e-03]\u001b[A\n",
            "Training:  37%|███▋      | 740/2000 [04:35<07:27,  2.82it/s, loss=4.4662, acc=0.258, ppl=87.0, lr=9.96e-03]\u001b[A\n",
            "Training:  37%|███▋      | 740/2000 [04:35<07:27,  2.82it/s, loss=4.3179, acc=0.269, ppl=75.0, lr=9.96e-03]\u001b[A\n",
            "Training:  38%|███▊      | 750/2000 [04:38<07:12,  2.89it/s, loss=4.3179, acc=0.269, ppl=75.0, lr=9.96e-03]\u001b[A\n",
            "Training:  38%|███▊      | 750/2000 [04:39<07:12,  2.89it/s, loss=4.1885, acc=0.294, ppl=65.9, lr=9.95e-03]\u001b[A\n",
            "Training:  38%|███▊      | 760/2000 [04:42<07:20,  2.82it/s, loss=4.1885, acc=0.294, ppl=65.9, lr=9.95e-03]\u001b[A\n",
            "Training:  38%|███▊      | 760/2000 [04:42<07:20,  2.82it/s, loss=4.3202, acc=0.271, ppl=75.2, lr=9.95e-03]\u001b[A\n",
            "Training:  38%|███▊      | 770/2000 [04:45<07:06,  2.89it/s, loss=4.3202, acc=0.271, ppl=75.2, lr=9.95e-03]\u001b[A\n",
            "Training:  38%|███▊      | 770/2000 [04:46<07:06,  2.89it/s, loss=4.1724, acc=0.289, ppl=64.9, lr=9.95e-03]\u001b[A\n",
            "Training:  39%|███▉      | 780/2000 [04:49<07:13,  2.82it/s, loss=4.1724, acc=0.289, ppl=64.9, lr=9.95e-03]\u001b[A\n",
            "Training:  39%|███▉      | 780/2000 [04:49<07:13,  2.82it/s, loss=4.1456, acc=0.281, ppl=63.2, lr=9.94e-03]\u001b[A\n",
            "Training:  40%|███▉      | 790/2000 [04:52<06:58,  2.89it/s, loss=4.1456, acc=0.281, ppl=63.2, lr=9.94e-03]\u001b[A\n",
            "Training:  40%|███▉      | 790/2000 [04:53<06:58,  2.89it/s, loss=4.1339, acc=0.293, ppl=62.4, lr=9.94e-03]\u001b[A\n",
            "Training:  40%|████      | 800/2000 [04:56<07:05,  2.82it/s, loss=4.1339, acc=0.293, ppl=62.4, lr=9.94e-03]\u001b[A\n",
            "Training:  40%|████      | 800/2000 [04:56<07:05,  2.82it/s, loss=4.2409, acc=0.274, ppl=69.5, lr=9.94e-03]\u001b[A\n",
            "Training:  40%|████      | 810/2000 [04:59<06:51,  2.89it/s, loss=4.2409, acc=0.274, ppl=69.5, lr=9.94e-03]\u001b[A\n",
            "Training:  40%|████      | 810/2000 [05:00<06:51,  2.89it/s, loss=4.1345, acc=0.293, ppl=62.5, lr=9.94e-03]\u001b[A\n",
            "Training:  41%|████      | 820/2000 [05:03<06:58,  2.82it/s, loss=4.1345, acc=0.293, ppl=62.5, lr=9.94e-03]\u001b[A\n",
            "Training:  41%|████      | 820/2000 [05:03<06:58,  2.82it/s, loss=4.0364, acc=0.305, ppl=56.6, lr=9.93e-03]\u001b[A\n",
            "Training:  42%|████▏     | 830/2000 [05:06<06:44,  2.89it/s, loss=4.0364, acc=0.305, ppl=56.6, lr=9.93e-03]\u001b[A\n",
            "Training:  42%|████▏     | 830/2000 [05:07<06:44,  2.89it/s, loss=4.0561, acc=0.299, ppl=57.8, lr=9.93e-03]\u001b[A\n",
            "Training:  42%|████▏     | 840/2000 [05:10<06:50,  2.82it/s, loss=4.0561, acc=0.299, ppl=57.8, lr=9.93e-03]\u001b[A\n",
            "Training:  42%|████▏     | 840/2000 [05:10<06:50,  2.82it/s, loss=3.9713, acc=0.309, ppl=53.1, lr=9.93e-03]\u001b[A\n",
            "Training:  42%|████▎     | 850/2000 [05:13<06:37,  2.89it/s, loss=3.9713, acc=0.309, ppl=53.1, lr=9.93e-03]\u001b[A\n",
            "Training:  42%|████▎     | 850/2000 [05:14<06:37,  2.89it/s, loss=4.0102, acc=0.299, ppl=55.2, lr=9.92e-03]\u001b[A\n",
            "Training:  43%|████▎     | 860/2000 [05:17<06:43,  2.82it/s, loss=4.0102, acc=0.299, ppl=55.2, lr=9.92e-03]\u001b[A\n",
            "Training:  43%|████▎     | 860/2000 [05:17<06:43,  2.82it/s, loss=3.9202, acc=0.309, ppl=50.4, lr=9.92e-03]\u001b[A\n",
            "Training:  44%|████▎     | 870/2000 [05:20<06:31,  2.89it/s, loss=3.9202, acc=0.309, ppl=50.4, lr=9.92e-03]\u001b[A\n",
            "Training:  44%|████▎     | 870/2000 [05:21<06:31,  2.89it/s, loss=3.9117, acc=0.306, ppl=50.0, lr=9.92e-03]\u001b[A\n",
            "Training:  44%|████▍     | 880/2000 [05:24<06:37,  2.82it/s, loss=3.9117, acc=0.306, ppl=50.0, lr=9.92e-03]\u001b[A\n",
            "Training:  44%|████▍     | 880/2000 [05:24<06:37,  2.82it/s, loss=3.7060, acc=0.340, ppl=40.7, lr=9.91e-03]\u001b[A\n",
            "Training:  44%|████▍     | 890/2000 [05:27<06:24,  2.89it/s, loss=3.7060, acc=0.340, ppl=40.7, lr=9.91e-03]\u001b[A\n",
            "Training:  44%|████▍     | 890/2000 [05:28<06:24,  2.89it/s, loss=3.8237, acc=0.323, ppl=45.8, lr=9.91e-03]\u001b[A\n",
            "Training:  45%|████▌     | 900/2000 [05:31<06:29,  2.82it/s, loss=3.8237, acc=0.323, ppl=45.8, lr=9.91e-03]\u001b[A\n",
            "Training:  45%|████▌     | 900/2000 [05:31<06:29,  2.82it/s, loss=3.7239, acc=0.327, ppl=41.4, lr=9.90e-03]\u001b[A\n",
            "Training:  46%|████▌     | 910/2000 [05:34<06:17,  2.89it/s, loss=3.7239, acc=0.327, ppl=41.4, lr=9.90e-03]\u001b[A\n",
            "Training:  46%|████▌     | 910/2000 [05:35<06:17,  2.89it/s, loss=3.9225, acc=0.316, ppl=50.5, lr=9.90e-03]\u001b[A\n",
            "Training:  46%|████▌     | 920/2000 [05:38<06:22,  2.82it/s, loss=3.9225, acc=0.316, ppl=50.5, lr=9.90e-03]\u001b[A\n",
            "Training:  46%|████▌     | 920/2000 [05:38<06:22,  2.82it/s, loss=3.6267, acc=0.347, ppl=37.6, lr=9.90e-03]\u001b[A\n",
            "Training:  46%|████▋     | 930/2000 [05:41<06:09,  2.89it/s, loss=3.6267, acc=0.347, ppl=37.6, lr=9.90e-03]\u001b[A\n",
            "Training:  46%|████▋     | 930/2000 [05:42<06:09,  2.89it/s, loss=3.8792, acc=0.306, ppl=48.4, lr=9.89e-03]\u001b[A\n",
            "Training:  47%|████▋     | 940/2000 [05:45<06:15,  2.82it/s, loss=3.8792, acc=0.306, ppl=48.4, lr=9.89e-03]\u001b[A\n",
            "Training:  47%|████▋     | 940/2000 [05:45<06:15,  2.82it/s, loss=3.7618, acc=0.326, ppl=43.0, lr=9.89e-03]\u001b[A\n",
            "Training:  48%|████▊     | 950/2000 [05:48<06:03,  2.89it/s, loss=3.7618, acc=0.326, ppl=43.0, lr=9.89e-03]\u001b[A\n",
            "Training:  48%|████▊     | 950/2000 [05:49<06:03,  2.89it/s, loss=3.7082, acc=0.335, ppl=40.8, lr=9.89e-03]\u001b[A\n",
            "Training:  48%|████▊     | 960/2000 [05:52<06:08,  2.82it/s, loss=3.7082, acc=0.335, ppl=40.8, lr=9.89e-03]\u001b[A\n",
            "Training:  48%|████▊     | 960/2000 [05:52<06:08,  2.82it/s, loss=3.6025, acc=0.346, ppl=36.7, lr=9.88e-03]\u001b[A\n",
            "Training:  48%|████▊     | 970/2000 [05:55<05:56,  2.89it/s, loss=3.6025, acc=0.346, ppl=36.7, lr=9.88e-03]\u001b[A\n",
            "Training:  48%|████▊     | 970/2000 [05:56<05:56,  2.89it/s, loss=3.4620, acc=0.358, ppl=31.9, lr=9.88e-03]\u001b[A\n",
            "Training:  49%|████▉     | 980/2000 [05:59<06:01,  2.82it/s, loss=3.4620, acc=0.358, ppl=31.9, lr=9.88e-03]\u001b[A\n",
            "Training:  49%|████▉     | 980/2000 [05:59<06:01,  2.82it/s, loss=3.5819, acc=0.356, ppl=35.9, lr=9.87e-03]\u001b[A\n",
            "Training:  50%|████▉     | 990/2000 [06:02<05:49,  2.89it/s, loss=3.5819, acc=0.356, ppl=35.9, lr=9.87e-03]\u001b[A\n",
            "Training:  50%|████▉     | 990/2000 [06:03<05:49,  2.89it/s, loss=3.4840, acc=0.364, ppl=32.6, lr=9.87e-03]\u001b[A\n",
            "Training:  50%|█████     | 1000/2000 [06:06<05:54,  2.82it/s, loss=3.4840, acc=0.364, ppl=32.6, lr=9.87e-03]\u001b[A\n",
            "Training:  50%|█████     | 1000/2000 [06:06<05:54,  2.82it/s, loss=3.5977, acc=0.356, ppl=36.5, lr=9.86e-03]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1000: Val Loss: 3.1843, Val Acc: 0.4083, Val PPL: 24.15\n",
            "💾 Saved best model with val_loss: 3.1843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Training:  50%|█████     | 1010/2000 [06:22<11:57,  1.38it/s, loss=3.5977, acc=0.356, ppl=36.5, lr=9.86e-03]\u001b[A\n",
            "Training:  50%|█████     | 1010/2000 [06:22<11:57,  1.38it/s, loss=3.4113, acc=0.375, ppl=30.3, lr=9.86e-03]\u001b[A\n",
            "Training:  51%|█████     | 1020/2000 [06:26<10:07,  1.61it/s, loss=3.4113, acc=0.375, ppl=30.3, lr=9.86e-03]\u001b[A\n",
            "Training:  51%|█████     | 1020/2000 [06:26<10:07,  1.61it/s, loss=3.2506, acc=0.399, ppl=25.8, lr=9.85e-03]\u001b[A\n",
            "Training:  52%|█████▏    | 1030/2000 [06:29<08:35,  1.88it/s, loss=3.2506, acc=0.399, ppl=25.8, lr=9.85e-03]\u001b[A\n",
            "Training:  52%|█████▏    | 1030/2000 [06:29<08:35,  1.88it/s, loss=3.4952, acc=0.364, ppl=33.0, lr=9.85e-03]\u001b[A\n",
            "Training:  52%|█████▏    | 1040/2000 [06:33<07:44,  2.07it/s, loss=3.4952, acc=0.364, ppl=33.0, lr=9.85e-03]\u001b[A\n",
            "Training:  52%|█████▏    | 1040/2000 [06:33<07:44,  2.07it/s, loss=3.5006, acc=0.365, ppl=33.1, lr=9.84e-03]\u001b[A\n",
            "Training:  52%|█████▎    | 1050/2000 [06:36<06:54,  2.29it/s, loss=3.5006, acc=0.365, ppl=33.1, lr=9.84e-03]\u001b[A\n",
            "Training:  52%|█████▎    | 1050/2000 [06:36<06:54,  2.29it/s, loss=3.3391, acc=0.384, ppl=28.2, lr=9.84e-03]\u001b[A\n",
            "Training:  53%|█████▎    | 1060/2000 [06:40<06:32,  2.39it/s, loss=3.3391, acc=0.384, ppl=28.2, lr=9.84e-03]\u001b[A\n",
            "Training:  53%|█████▎    | 1060/2000 [06:40<06:32,  2.39it/s, loss=3.4824, acc=0.365, ppl=32.5, lr=9.83e-03]\u001b[A\n",
            "Training:  54%|█████▎    | 1070/2000 [06:43<06:02,  2.56it/s, loss=3.4824, acc=0.365, ppl=32.5, lr=9.83e-03]\u001b[A\n",
            "Training:  54%|█████▎    | 1070/2000 [06:43<06:02,  2.56it/s, loss=3.4526, acc=0.361, ppl=31.6, lr=9.83e-03]\u001b[A\n",
            "Training:  54%|█████▍    | 1080/2000 [06:47<05:54,  2.59it/s, loss=3.4526, acc=0.361, ppl=31.6, lr=9.83e-03]\u001b[A\n",
            "Training:  54%|█████▍    | 1080/2000 [06:47<05:54,  2.59it/s, loss=3.3292, acc=0.373, ppl=27.9, lr=9.82e-03]\u001b[A\n",
            "Training:  55%|█████▍    | 1090/2000 [06:50<05:34,  2.72it/s, loss=3.3292, acc=0.373, ppl=27.9, lr=9.82e-03]\u001b[A\n",
            "Training:  55%|█████▍    | 1090/2000 [06:50<05:34,  2.72it/s, loss=3.2724, acc=0.391, ppl=26.4, lr=9.82e-03]\u001b[A\n",
            "Training:  55%|█████▌    | 1100/2000 [06:54<05:32,  2.70it/s, loss=3.2724, acc=0.391, ppl=26.4, lr=9.82e-03]\u001b[A\n",
            "Training:  55%|█████▌    | 1100/2000 [06:54<05:32,  2.70it/s, loss=3.1419, acc=0.408, ppl=23.1, lr=9.81e-03]\u001b[A\n",
            "Training:  56%|█████▌    | 1110/2000 [06:57<05:17,  2.81it/s, loss=3.1419, acc=0.408, ppl=23.1, lr=9.81e-03]\u001b[A\n",
            "Training:  56%|█████▌    | 1110/2000 [06:57<05:17,  2.81it/s, loss=3.2348, acc=0.389, ppl=25.4, lr=9.81e-03]\u001b[A\n",
            "Training:  56%|█████▌    | 1120/2000 [07:01<05:18,  2.76it/s, loss=3.2348, acc=0.389, ppl=25.4, lr=9.81e-03]\u001b[A\n",
            "Training:  56%|█████▌    | 1120/2000 [07:01<05:18,  2.76it/s, loss=3.4018, acc=0.367, ppl=30.0, lr=9.80e-03]\u001b[A\n",
            "Training:  56%|█████▋    | 1130/2000 [07:04<05:05,  2.85it/s, loss=3.4018, acc=0.367, ppl=30.0, lr=9.80e-03]\u001b[A\n",
            "Training:  56%|█████▋    | 1130/2000 [07:04<05:05,  2.85it/s, loss=3.1875, acc=0.404, ppl=24.2, lr=9.80e-03]\u001b[A\n",
            "Training:  57%|█████▋    | 1140/2000 [07:08<05:08,  2.79it/s, loss=3.1875, acc=0.404, ppl=24.2, lr=9.80e-03]\u001b[A\n",
            "Training:  57%|█████▋    | 1140/2000 [07:08<05:08,  2.79it/s, loss=3.2512, acc=0.396, ppl=25.8, lr=9.79e-03]\u001b[A\n",
            "Training:  57%|█████▊    | 1150/2000 [07:11<04:56,  2.87it/s, loss=3.2512, acc=0.396, ppl=25.8, lr=9.79e-03]\u001b[A\n",
            "Training:  57%|█████▊    | 1150/2000 [07:11<04:56,  2.87it/s, loss=3.3681, acc=0.366, ppl=29.0, lr=9.79e-03]\u001b[A\n",
            "Training:  58%|█████▊    | 1160/2000 [07:15<04:59,  2.81it/s, loss=3.3681, acc=0.366, ppl=29.0, lr=9.79e-03]\u001b[A\n",
            "Training:  58%|█████▊    | 1160/2000 [07:15<04:59,  2.81it/s, loss=3.0060, acc=0.433, ppl=20.2, lr=9.78e-03]\u001b[A\n",
            "Training:  58%|█████▊    | 1170/2000 [07:18<04:47,  2.88it/s, loss=3.0060, acc=0.433, ppl=20.2, lr=9.78e-03]\u001b[A\n",
            "Training:  58%|█████▊    | 1170/2000 [07:18<04:47,  2.88it/s, loss=3.0225, acc=0.419, ppl=20.5, lr=9.78e-03]\u001b[A\n",
            "Training:  59%|█████▉    | 1180/2000 [07:22<04:50,  2.82it/s, loss=3.0225, acc=0.419, ppl=20.5, lr=9.78e-03]\u001b[A\n",
            "Training:  59%|█████▉    | 1180/2000 [07:22<04:50,  2.82it/s, loss=2.9936, acc=0.423, ppl=20.0, lr=9.77e-03]\u001b[A\n",
            "Training:  60%|█████▉    | 1190/2000 [07:25<04:40,  2.89it/s, loss=2.9936, acc=0.423, ppl=20.0, lr=9.77e-03]\u001b[A\n",
            "Training:  60%|█████▉    | 1190/2000 [07:25<04:40,  2.89it/s, loss=3.1834, acc=0.403, ppl=24.1, lr=9.76e-03]\u001b[A\n",
            "Training:  60%|██████    | 1200/2000 [07:29<04:43,  2.82it/s, loss=3.1834, acc=0.403, ppl=24.1, lr=9.76e-03]\u001b[A\n",
            "Training:  60%|██████    | 1200/2000 [07:29<04:43,  2.82it/s, loss=3.0383, acc=0.418, ppl=20.9, lr=9.76e-03]\u001b[A\n",
            "Training:  60%|██████    | 1210/2000 [07:32<04:33,  2.89it/s, loss=3.0383, acc=0.418, ppl=20.9, lr=9.76e-03]\u001b[A\n",
            "Training:  60%|██████    | 1210/2000 [07:32<04:33,  2.89it/s, loss=2.9351, acc=0.431, ppl=18.8, lr=9.75e-03]\u001b[A\n",
            "Training:  61%|██████    | 1220/2000 [07:36<04:36,  2.82it/s, loss=2.9351, acc=0.431, ppl=18.8, lr=9.75e-03]\u001b[A\n",
            "Training:  61%|██████    | 1220/2000 [07:36<04:36,  2.82it/s, loss=3.0036, acc=0.426, ppl=20.2, lr=9.74e-03]\u001b[A\n",
            "Training:  62%|██████▏   | 1230/2000 [07:39<04:26,  2.89it/s, loss=3.0036, acc=0.426, ppl=20.2, lr=9.74e-03]\u001b[A\n",
            "Training:  62%|██████▏   | 1230/2000 [07:39<04:26,  2.89it/s, loss=3.1790, acc=0.403, ppl=24.0, lr=9.74e-03]\u001b[A\n",
            "Training:  62%|██████▏   | 1240/2000 [07:43<04:29,  2.82it/s, loss=3.1790, acc=0.403, ppl=24.0, lr=9.74e-03]\u001b[A\n",
            "Training:  62%|██████▏   | 1240/2000 [07:43<04:29,  2.82it/s, loss=3.0776, acc=0.411, ppl=21.7, lr=9.73e-03]\u001b[A\n",
            "Training:  62%|██████▎   | 1250/2000 [07:46<04:19,  2.89it/s, loss=3.0776, acc=0.411, ppl=21.7, lr=9.73e-03]\u001b[A\n",
            "Training:  62%|██████▎   | 1250/2000 [07:46<04:19,  2.89it/s, loss=2.9592, acc=0.421, ppl=19.3, lr=9.73e-03]\u001b[A\n",
            "Training:  63%|██████▎   | 1260/2000 [07:50<04:22,  2.82it/s, loss=2.9592, acc=0.421, ppl=19.3, lr=9.73e-03]\u001b[A\n",
            "Training:  63%|██████▎   | 1260/2000 [07:50<04:22,  2.82it/s, loss=2.9371, acc=0.440, ppl=18.9, lr=9.72e-03]\u001b[A\n",
            "Training:  64%|██████▎   | 1270/2000 [07:53<04:12,  2.89it/s, loss=2.9371, acc=0.440, ppl=18.9, lr=9.72e-03]\u001b[A\n",
            "Training:  64%|██████▎   | 1270/2000 [07:53<04:12,  2.89it/s, loss=2.8596, acc=0.446, ppl=17.5, lr=9.71e-03]\u001b[A\n",
            "Training:  64%|██████▍   | 1280/2000 [07:57<04:14,  2.82it/s, loss=2.8596, acc=0.446, ppl=17.5, lr=9.71e-03]\u001b[A\n",
            "Training:  64%|██████▍   | 1280/2000 [07:57<04:14,  2.82it/s, loss=2.9448, acc=0.425, ppl=19.0, lr=9.71e-03]\u001b[A\n",
            "Training:  64%|██████▍   | 1290/2000 [08:00<04:05,  2.89it/s, loss=2.9448, acc=0.425, ppl=19.0, lr=9.71e-03]\u001b[A\n",
            "Training:  64%|██████▍   | 1290/2000 [08:00<04:05,  2.89it/s, loss=2.8924, acc=0.441, ppl=18.0, lr=9.70e-03]\u001b[A\n",
            "Training:  65%|██████▌   | 1300/2000 [08:04<04:07,  2.83it/s, loss=2.8924, acc=0.441, ppl=18.0, lr=9.70e-03]\u001b[A\n",
            "Training:  65%|██████▌   | 1300/2000 [08:04<04:07,  2.83it/s, loss=2.8265, acc=0.447, ppl=16.9, lr=9.69e-03]\u001b[A\n",
            "Training:  66%|██████▌   | 1310/2000 [08:07<03:58,  2.90it/s, loss=2.8265, acc=0.447, ppl=16.9, lr=9.69e-03]\u001b[A\n",
            "Training:  66%|██████▌   | 1310/2000 [08:07<03:58,  2.90it/s, loss=2.7899, acc=0.452, ppl=16.3, lr=9.69e-03]\u001b[A\n",
            "Training:  66%|██████▌   | 1320/2000 [08:11<04:00,  2.83it/s, loss=2.7899, acc=0.452, ppl=16.3, lr=9.69e-03]\u001b[A\n",
            "Training:  66%|██████▌   | 1320/2000 [08:11<04:00,  2.83it/s, loss=2.8318, acc=0.447, ppl=17.0, lr=9.68e-03]\u001b[A\n",
            "Training:  66%|██████▋   | 1330/2000 [08:14<03:51,  2.90it/s, loss=2.8318, acc=0.447, ppl=17.0, lr=9.68e-03]\u001b[A\n",
            "Training:  66%|██████▋   | 1330/2000 [08:14<03:51,  2.90it/s, loss=2.7811, acc=0.455, ppl=16.1, lr=9.67e-03]\u001b[A\n",
            "Training:  67%|██████▋   | 1340/2000 [08:18<03:53,  2.83it/s, loss=2.7811, acc=0.455, ppl=16.1, lr=9.67e-03]\u001b[A\n",
            "Training:  67%|██████▋   | 1340/2000 [08:18<03:53,  2.83it/s, loss=2.7614, acc=0.458, ppl=15.8, lr=9.66e-03]\u001b[A\n",
            "Training:  68%|██████▊   | 1350/2000 [08:21<03:44,  2.90it/s, loss=2.7614, acc=0.458, ppl=15.8, lr=9.66e-03]\u001b[A\n",
            "Training:  68%|██████▊   | 1350/2000 [08:21<03:44,  2.90it/s, loss=2.6815, acc=0.472, ppl=14.6, lr=9.66e-03]\u001b[A\n",
            "Training:  68%|██████▊   | 1360/2000 [08:25<03:46,  2.83it/s, loss=2.6815, acc=0.472, ppl=14.6, lr=9.66e-03]\u001b[A\n",
            "Training:  68%|██████▊   | 1360/2000 [08:25<03:46,  2.83it/s, loss=2.7644, acc=0.464, ppl=15.9, lr=9.65e-03]\u001b[A\n",
            "Training:  68%|██████▊   | 1370/2000 [08:28<03:37,  2.90it/s, loss=2.7644, acc=0.464, ppl=15.9, lr=9.65e-03]\u001b[A\n",
            "Training:  68%|██████▊   | 1370/2000 [08:28<03:37,  2.90it/s, loss=2.6724, acc=0.472, ppl=14.5, lr=9.64e-03]\u001b[A\n",
            "Training:  69%|██████▉   | 1380/2000 [08:32<03:39,  2.83it/s, loss=2.6724, acc=0.472, ppl=14.5, lr=9.64e-03]\u001b[A\n",
            "Training:  69%|██████▉   | 1380/2000 [08:32<03:39,  2.83it/s, loss=2.6958, acc=0.468, ppl=14.8, lr=9.64e-03]\u001b[A\n",
            "Training:  70%|██████▉   | 1390/2000 [08:35<03:30,  2.90it/s, loss=2.6958, acc=0.468, ppl=14.8, lr=9.64e-03]\u001b[A\n",
            "Training:  70%|██████▉   | 1390/2000 [08:35<03:30,  2.90it/s, loss=2.5591, acc=0.489, ppl=12.9, lr=9.63e-03]\u001b[A\n",
            "Training:  70%|███████   | 1400/2000 [08:39<03:32,  2.83it/s, loss=2.5591, acc=0.489, ppl=12.9, lr=9.63e-03]\u001b[A\n",
            "Training:  70%|███████   | 1400/2000 [08:39<03:32,  2.83it/s, loss=2.6381, acc=0.489, ppl=14.0, lr=9.62e-03]\u001b[A\n",
            "Training:  70%|███████   | 1410/2000 [08:42<03:23,  2.90it/s, loss=2.6381, acc=0.489, ppl=14.0, lr=9.62e-03]\u001b[A\n",
            "Training:  70%|███████   | 1410/2000 [08:42<03:23,  2.90it/s, loss=2.4953, acc=0.500, ppl=12.1, lr=9.61e-03]\u001b[A\n",
            "Training:  71%|███████   | 1420/2000 [08:46<03:25,  2.83it/s, loss=2.4953, acc=0.500, ppl=12.1, lr=9.61e-03]\u001b[A\n",
            "Training:  71%|███████   | 1420/2000 [08:46<03:25,  2.83it/s, loss=2.6100, acc=0.480, ppl=13.6, lr=9.61e-03]\u001b[A\n",
            "Training:  72%|███████▏  | 1430/2000 [08:49<03:16,  2.90it/s, loss=2.6100, acc=0.480, ppl=13.6, lr=9.61e-03]\u001b[A\n",
            "Training:  72%|███████▏  | 1430/2000 [08:49<03:16,  2.90it/s, loss=2.5453, acc=0.488, ppl=12.7, lr=9.60e-03]\u001b[A\n",
            "Training:  72%|███████▏  | 1440/2000 [08:53<03:18,  2.83it/s, loss=2.5453, acc=0.488, ppl=12.7, lr=9.60e-03]\u001b[A\n",
            "Training:  72%|███████▏  | 1440/2000 [08:53<03:18,  2.83it/s, loss=2.5469, acc=0.492, ppl=12.8, lr=9.59e-03]\u001b[A\n",
            "Training:  72%|███████▎  | 1450/2000 [08:56<03:09,  2.90it/s, loss=2.5469, acc=0.492, ppl=12.8, lr=9.59e-03]\u001b[A\n",
            "Training:  72%|███████▎  | 1450/2000 [08:56<03:09,  2.90it/s, loss=2.5785, acc=0.482, ppl=13.2, lr=9.58e-03]\u001b[A\n",
            "Training:  73%|███████▎  | 1460/2000 [09:00<03:11,  2.82it/s, loss=2.5785, acc=0.482, ppl=13.2, lr=9.58e-03]\u001b[A\n",
            "Training:  73%|███████▎  | 1460/2000 [09:00<03:11,  2.82it/s, loss=2.5901, acc=0.484, ppl=13.3, lr=9.57e-03]\u001b[A\n",
            "Training:  74%|███████▎  | 1470/2000 [09:03<03:03,  2.89it/s, loss=2.5901, acc=0.484, ppl=13.3, lr=9.57e-03]\u001b[A\n",
            "Training:  74%|███████▎  | 1470/2000 [09:03<03:03,  2.89it/s, loss=2.6161, acc=0.468, ppl=13.7, lr=9.57e-03]\u001b[A\n",
            "Training:  74%|███████▍  | 1480/2000 [09:07<03:04,  2.82it/s, loss=2.6161, acc=0.468, ppl=13.7, lr=9.57e-03]\u001b[A\n",
            "Training:  74%|███████▍  | 1480/2000 [09:07<03:04,  2.82it/s, loss=2.4042, acc=0.513, ppl=11.1, lr=9.56e-03]\u001b[A\n",
            "Training:  74%|███████▍  | 1490/2000 [09:10<02:56,  2.89it/s, loss=2.4042, acc=0.513, ppl=11.1, lr=9.56e-03]\u001b[A\n",
            "Training:  74%|███████▍  | 1490/2000 [09:10<02:56,  2.89it/s, loss=2.5656, acc=0.490, ppl=13.0, lr=9.55e-03]\u001b[A\n",
            "Training:  75%|███████▌  | 1500/2000 [09:14<02:57,  2.82it/s, loss=2.5656, acc=0.490, ppl=13.0, lr=9.55e-03]\u001b[A\n",
            "Training:  75%|███████▌  | 1500/2000 [09:14<02:57,  2.82it/s, loss=2.5132, acc=0.495, ppl=12.3, lr=9.54e-03]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1500: Val Loss: 1.8982, Val Acc: 0.6076, Val PPL: 6.67\n",
            "💾 Saved best model with val_loss: 1.8982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Training:  76%|███████▌  | 1510/2000 [09:29<05:55,  1.38it/s, loss=2.5132, acc=0.495, ppl=12.3, lr=9.54e-03]\u001b[A\n",
            "Training:  76%|███████▌  | 1510/2000 [09:30<05:55,  1.38it/s, loss=2.2681, acc=0.533, ppl=9.7, lr=9.54e-03] \u001b[A\n",
            "Training:  76%|███████▌  | 1520/2000 [09:33<04:57,  1.61it/s, loss=2.2681, acc=0.533, ppl=9.7, lr=9.54e-03]\u001b[A\n",
            "Training:  76%|███████▌  | 1520/2000 [09:34<04:57,  1.61it/s, loss=2.3572, acc=0.517, ppl=10.6, lr=9.53e-03]\u001b[A\n",
            "Training:  76%|███████▋  | 1530/2000 [09:36<04:09,  1.88it/s, loss=2.3572, acc=0.517, ppl=10.6, lr=9.53e-03]\u001b[A\n",
            "Training:  76%|███████▋  | 1530/2000 [09:37<04:09,  1.88it/s, loss=2.4870, acc=0.494, ppl=12.0, lr=9.52e-03]\u001b[A\n",
            "Training:  77%|███████▋  | 1540/2000 [09:40<03:42,  2.07it/s, loss=2.4870, acc=0.494, ppl=12.0, lr=9.52e-03]\u001b[A\n",
            "Training:  77%|███████▋  | 1540/2000 [09:41<03:42,  2.07it/s, loss=2.3369, acc=0.515, ppl=10.3, lr=9.51e-03]\u001b[A\n",
            "Training:  78%|███████▊  | 1550/2000 [09:43<03:16,  2.29it/s, loss=2.3369, acc=0.515, ppl=10.3, lr=9.51e-03]\u001b[A\n",
            "Training:  78%|███████▊  | 1550/2000 [09:44<03:16,  2.29it/s, loss=2.4015, acc=0.509, ppl=11.0, lr=9.50e-03]\u001b[A\n",
            "Training:  78%|███████▊  | 1560/2000 [09:47<03:03,  2.39it/s, loss=2.4015, acc=0.509, ppl=11.0, lr=9.50e-03]\u001b[A\n",
            "Training:  78%|███████▊  | 1560/2000 [09:48<03:03,  2.39it/s, loss=2.2472, acc=0.535, ppl=9.5, lr=9.49e-03] \u001b[A\n",
            "Training:  78%|███████▊  | 1570/2000 [09:50<02:47,  2.57it/s, loss=2.2472, acc=0.535, ppl=9.5, lr=9.49e-03]\u001b[A\n",
            "Training:  78%|███████▊  | 1570/2000 [09:51<02:47,  2.57it/s, loss=2.3209, acc=0.519, ppl=10.2, lr=9.49e-03]\u001b[A\n",
            "Training:  79%|███████▉  | 1580/2000 [09:54<02:41,  2.60it/s, loss=2.3209, acc=0.519, ppl=10.2, lr=9.49e-03]\u001b[A\n",
            "Training:  79%|███████▉  | 1580/2000 [09:55<02:41,  2.60it/s, loss=2.3819, acc=0.510, ppl=10.8, lr=9.48e-03]\u001b[A\n",
            "Training:  80%|███████▉  | 1590/2000 [09:57<02:30,  2.72it/s, loss=2.3819, acc=0.510, ppl=10.8, lr=9.48e-03]\u001b[A\n",
            "Training:  80%|███████▉  | 1590/2000 [09:58<02:30,  2.72it/s, loss=2.2558, acc=0.539, ppl=9.5, lr=9.47e-03] \u001b[A\n",
            "Training:  80%|████████  | 1600/2000 [10:01<02:27,  2.71it/s, loss=2.2558, acc=0.539, ppl=9.5, lr=9.47e-03]\u001b[A\n",
            "Training:  80%|████████  | 1600/2000 [10:01<02:27,  2.71it/s, loss=2.3358, acc=0.519, ppl=10.3, lr=9.46e-03]\u001b[A\n",
            "Training:  80%|████████  | 1610/2000 [10:04<02:18,  2.81it/s, loss=2.3358, acc=0.519, ppl=10.3, lr=9.46e-03]\u001b[A\n",
            "Training:  80%|████████  | 1610/2000 [10:05<02:18,  2.81it/s, loss=2.2584, acc=0.534, ppl=9.6, lr=9.45e-03] \u001b[A\n",
            "Training:  81%|████████  | 1620/2000 [10:08<02:17,  2.77it/s, loss=2.2584, acc=0.534, ppl=9.6, lr=9.45e-03]\u001b[A\n",
            "Training:  81%|████████  | 1620/2000 [10:08<02:17,  2.77it/s, loss=2.4109, acc=0.505, ppl=11.1, lr=9.44e-03]\u001b[A\n",
            "Training:  82%|████████▏ | 1630/2000 [10:11<02:09,  2.85it/s, loss=2.4109, acc=0.505, ppl=11.1, lr=9.44e-03]\u001b[A\n",
            "Training:  82%|████████▏ | 1630/2000 [10:12<02:09,  2.85it/s, loss=2.2535, acc=0.522, ppl=9.5, lr=9.43e-03] \u001b[A\n",
            "Training:  82%|████████▏ | 1640/2000 [10:15<02:08,  2.80it/s, loss=2.2535, acc=0.522, ppl=9.5, lr=9.43e-03]\u001b[A\n",
            "Training:  82%|████████▏ | 1640/2000 [10:15<02:08,  2.80it/s, loss=2.2442, acc=0.534, ppl=9.4, lr=9.42e-03]\u001b[A\n",
            "Training:  82%|████████▎ | 1650/2000 [10:18<02:01,  2.87it/s, loss=2.2442, acc=0.534, ppl=9.4, lr=9.42e-03]\u001b[A\n",
            "Training:  82%|████████▎ | 1650/2000 [10:19<02:01,  2.87it/s, loss=2.1595, acc=0.553, ppl=8.7, lr=9.41e-03]\u001b[A\n",
            "Training:  83%|████████▎ | 1660/2000 [10:22<02:00,  2.81it/s, loss=2.1595, acc=0.553, ppl=8.7, lr=9.41e-03]\u001b[A\n",
            "Training:  83%|████████▎ | 1660/2000 [10:22<02:00,  2.81it/s, loss=2.1308, acc=0.554, ppl=8.4, lr=9.40e-03]\u001b[A\n",
            "Training:  84%|████████▎ | 1670/2000 [10:25<01:54,  2.89it/s, loss=2.1308, acc=0.554, ppl=8.4, lr=9.40e-03]\u001b[A\n",
            "Training:  84%|████████▎ | 1670/2000 [10:26<01:54,  2.89it/s, loss=2.2109, acc=0.534, ppl=9.1, lr=9.40e-03]\u001b[A\n",
            "Training:  84%|████████▍ | 1680/2000 [10:29<01:53,  2.82it/s, loss=2.2109, acc=0.534, ppl=9.1, lr=9.40e-03]\u001b[A\n",
            "Training:  84%|████████▍ | 1680/2000 [10:29<01:53,  2.82it/s, loss=2.3020, acc=0.520, ppl=10.0, lr=9.38e-03]\u001b[A\n",
            "Training:  84%|████████▍ | 1690/2000 [10:32<01:47,  2.90it/s, loss=2.3020, acc=0.520, ppl=10.0, lr=9.38e-03]\u001b[A\n",
            "Training:  84%|████████▍ | 1690/2000 [10:33<01:47,  2.90it/s, loss=2.2421, acc=0.532, ppl=9.4, lr=9.38e-03] \u001b[A\n",
            "Training:  85%|████████▌ | 1700/2000 [10:36<01:46,  2.83it/s, loss=2.2421, acc=0.532, ppl=9.4, lr=9.38e-03]\u001b[A\n",
            "Training:  85%|████████▌ | 1700/2000 [10:36<01:46,  2.83it/s, loss=2.2420, acc=0.531, ppl=9.4, lr=9.37e-03]\u001b[A\n",
            "Training:  86%|████████▌ | 1710/2000 [10:39<01:39,  2.90it/s, loss=2.2420, acc=0.531, ppl=9.4, lr=9.37e-03]\u001b[A\n",
            "Training:  86%|████████▌ | 1710/2000 [10:40<01:39,  2.90it/s, loss=2.1799, acc=0.547, ppl=8.8, lr=9.36e-03]\u001b[A\n",
            "Training:  86%|████████▌ | 1720/2000 [10:43<01:38,  2.83it/s, loss=2.1799, acc=0.547, ppl=8.8, lr=9.36e-03]\u001b[A\n",
            "Training:  86%|████████▌ | 1720/2000 [10:43<01:38,  2.83it/s, loss=2.1887, acc=0.547, ppl=8.9, lr=9.35e-03]\u001b[A\n",
            "Training:  86%|████████▋ | 1730/2000 [10:46<01:32,  2.90it/s, loss=2.1887, acc=0.547, ppl=8.9, lr=9.35e-03]\u001b[A\n",
            "Training:  86%|████████▋ | 1730/2000 [10:47<01:32,  2.90it/s, loss=2.0364, acc=0.574, ppl=7.7, lr=9.34e-03]\u001b[A\n",
            "Training:  87%|████████▋ | 1740/2000 [10:50<01:31,  2.84it/s, loss=2.0364, acc=0.574, ppl=7.7, lr=9.34e-03]\u001b[A\n",
            "Training:  87%|████████▋ | 1740/2000 [10:50<01:31,  2.84it/s, loss=2.1975, acc=0.534, ppl=9.0, lr=9.33e-03]\u001b[A\n",
            "Training:  88%|████████▊ | 1750/2000 [10:53<01:26,  2.91it/s, loss=2.1975, acc=0.534, ppl=9.0, lr=9.33e-03]\u001b[A\n",
            "Training:  88%|████████▊ | 1750/2000 [10:54<01:26,  2.91it/s, loss=1.9547, acc=0.585, ppl=7.1, lr=9.32e-03]\u001b[A\n",
            "Training:  88%|████████▊ | 1760/2000 [10:57<01:24,  2.84it/s, loss=1.9547, acc=0.585, ppl=7.1, lr=9.32e-03]\u001b[A\n",
            "Training:  88%|████████▊ | 1760/2000 [10:57<01:24,  2.84it/s, loss=2.1117, acc=0.548, ppl=8.3, lr=9.31e-03]\u001b[A\n",
            "Training:  88%|████████▊ | 1770/2000 [11:00<01:19,  2.91it/s, loss=2.1117, acc=0.548, ppl=8.3, lr=9.31e-03]\u001b[A\n",
            "Training:  88%|████████▊ | 1770/2000 [11:01<01:19,  2.91it/s, loss=1.9710, acc=0.584, ppl=7.2, lr=9.30e-03]\u001b[A\n",
            "Training:  89%|████████▉ | 1780/2000 [11:04<01:17,  2.84it/s, loss=1.9710, acc=0.584, ppl=7.2, lr=9.30e-03]\u001b[A\n",
            "Training:  89%|████████▉ | 1780/2000 [11:04<01:17,  2.84it/s, loss=1.9590, acc=0.583, ppl=7.1, lr=9.29e-03]\u001b[A\n",
            "Training:  90%|████████▉ | 1790/2000 [11:07<01:12,  2.91it/s, loss=1.9590, acc=0.583, ppl=7.1, lr=9.29e-03]\u001b[A\n",
            "Training:  90%|████████▉ | 1790/2000 [11:08<01:12,  2.91it/s, loss=2.0607, acc=0.560, ppl=7.9, lr=9.28e-03]\u001b[A\n",
            "Training:  90%|█████████ | 1800/2000 [11:11<01:10,  2.84it/s, loss=2.0607, acc=0.560, ppl=7.9, lr=9.28e-03]\u001b[A\n",
            "Training:  90%|█████████ | 1800/2000 [11:11<01:10,  2.84it/s, loss=1.8824, acc=0.595, ppl=6.6, lr=9.27e-03]\u001b[A\n",
            "Training:  90%|█████████ | 1810/2000 [11:14<01:05,  2.90it/s, loss=1.8824, acc=0.595, ppl=6.6, lr=9.27e-03]\u001b[A\n",
            "Training:  90%|█████████ | 1810/2000 [11:15<01:05,  2.90it/s, loss=2.0774, acc=0.558, ppl=8.0, lr=9.26e-03]\u001b[A\n",
            "Training:  91%|█████████ | 1820/2000 [11:18<01:03,  2.83it/s, loss=2.0774, acc=0.558, ppl=8.0, lr=9.26e-03]\u001b[A\n",
            "Training:  91%|█████████ | 1820/2000 [11:18<01:03,  2.83it/s, loss=1.9960, acc=0.571, ppl=7.4, lr=9.25e-03]\u001b[A\n",
            "Training:  92%|█████████▏| 1830/2000 [11:21<00:58,  2.90it/s, loss=1.9960, acc=0.571, ppl=7.4, lr=9.25e-03]\u001b[A\n",
            "Training:  92%|█████████▏| 1830/2000 [11:22<00:58,  2.90it/s, loss=1.8706, acc=0.595, ppl=6.5, lr=9.24e-03]\u001b[A\n",
            "Training:  92%|█████████▏| 1840/2000 [11:25<00:56,  2.83it/s, loss=1.8706, acc=0.595, ppl=6.5, lr=9.24e-03]\u001b[A\n",
            "Training:  92%|█████████▏| 1840/2000 [11:25<00:56,  2.83it/s, loss=1.9400, acc=0.582, ppl=7.0, lr=9.23e-03]\u001b[A\n",
            "Training:  92%|█████████▎| 1850/2000 [11:28<00:51,  2.90it/s, loss=1.9400, acc=0.582, ppl=7.0, lr=9.23e-03]\u001b[A\n",
            "Training:  92%|█████████▎| 1850/2000 [11:29<00:51,  2.90it/s, loss=2.1076, acc=0.557, ppl=8.2, lr=9.22e-03]\u001b[A\n",
            "Training:  93%|█████████▎| 1860/2000 [11:32<00:49,  2.83it/s, loss=2.1076, acc=0.557, ppl=8.2, lr=9.22e-03]\u001b[A\n",
            "Training:  93%|█████████▎| 1860/2000 [11:32<00:49,  2.83it/s, loss=1.9459, acc=0.588, ppl=7.0, lr=9.21e-03]\u001b[A\n",
            "Training:  94%|█████████▎| 1870/2000 [11:35<00:44,  2.91it/s, loss=1.9459, acc=0.588, ppl=7.0, lr=9.21e-03]\u001b[A\n",
            "Training:  94%|█████████▎| 1870/2000 [11:36<00:44,  2.91it/s, loss=2.0563, acc=0.565, ppl=7.8, lr=9.20e-03]\u001b[A\n",
            "Training:  94%|█████████▍| 1880/2000 [11:39<00:42,  2.83it/s, loss=2.0563, acc=0.565, ppl=7.8, lr=9.20e-03]\u001b[A\n",
            "Training:  94%|█████████▍| 1880/2000 [11:39<00:42,  2.83it/s, loss=1.8382, acc=0.595, ppl=6.3, lr=9.18e-03]\u001b[A\n",
            "Training:  94%|█████████▍| 1890/2000 [11:42<00:37,  2.90it/s, loss=1.8382, acc=0.595, ppl=6.3, lr=9.18e-03]\u001b[A\n",
            "Training:  94%|█████████▍| 1890/2000 [11:43<00:37,  2.90it/s, loss=1.9835, acc=0.567, ppl=7.3, lr=9.18e-03]\u001b[A\n",
            "Training:  95%|█████████▌| 1900/2000 [11:46<00:35,  2.83it/s, loss=1.9835, acc=0.567, ppl=7.3, lr=9.18e-03]\u001b[A\n",
            "Training:  95%|█████████▌| 1900/2000 [11:46<00:35,  2.83it/s, loss=1.9766, acc=0.577, ppl=7.2, lr=9.16e-03]\u001b[A\n",
            "Training:  96%|█████████▌| 1910/2000 [11:49<00:31,  2.90it/s, loss=1.9766, acc=0.577, ppl=7.2, lr=9.16e-03]\u001b[A\n",
            "Training:  96%|█████████▌| 1910/2000 [11:50<00:31,  2.90it/s, loss=1.8289, acc=0.600, ppl=6.2, lr=9.15e-03]\u001b[A\n",
            "Training:  96%|█████████▌| 1920/2000 [11:53<00:28,  2.83it/s, loss=1.8289, acc=0.600, ppl=6.2, lr=9.15e-03]\u001b[A\n",
            "Training:  96%|█████████▌| 1920/2000 [11:53<00:28,  2.83it/s, loss=1.7959, acc=0.608, ppl=6.0, lr=9.14e-03]\u001b[A\n",
            "Training:  96%|█████████▋| 1930/2000 [11:56<00:24,  2.90it/s, loss=1.7959, acc=0.608, ppl=6.0, lr=9.14e-03]\u001b[A\n",
            "Training:  96%|█████████▋| 1930/2000 [11:57<00:24,  2.90it/s, loss=1.8473, acc=0.592, ppl=6.3, lr=9.13e-03]\u001b[A\n",
            "Training:  97%|█████████▋| 1940/2000 [12:00<00:21,  2.83it/s, loss=1.8473, acc=0.592, ppl=6.3, lr=9.13e-03]\u001b[A\n",
            "Training:  97%|█████████▋| 1940/2000 [12:00<00:21,  2.83it/s, loss=1.7758, acc=0.612, ppl=5.9, lr=9.12e-03]\u001b[A\n",
            "Training:  98%|█████████▊| 1950/2000 [12:03<00:17,  2.90it/s, loss=1.7758, acc=0.612, ppl=5.9, lr=9.12e-03]\u001b[A\n",
            "Training:  98%|█████████▊| 1950/2000 [12:04<00:17,  2.90it/s, loss=1.8394, acc=0.596, ppl=6.3, lr=9.11e-03]\u001b[A\n",
            "Training:  98%|█████████▊| 1960/2000 [12:07<00:14,  2.83it/s, loss=1.8394, acc=0.596, ppl=6.3, lr=9.11e-03]\u001b[A\n",
            "Training:  98%|█████████▊| 1960/2000 [12:07<00:14,  2.83it/s, loss=1.7139, acc=0.615, ppl=5.6, lr=9.10e-03]\u001b[A\n",
            "Training:  98%|█████████▊| 1970/2000 [12:10<00:10,  2.90it/s, loss=1.7139, acc=0.615, ppl=5.6, lr=9.10e-03]\u001b[A\n",
            "Training:  98%|█████████▊| 1970/2000 [12:11<00:10,  2.90it/s, loss=1.8138, acc=0.598, ppl=6.1, lr=9.09e-03]\u001b[A\n",
            "Training:  99%|█████████▉| 1980/2000 [12:14<00:07,  2.83it/s, loss=1.8138, acc=0.598, ppl=6.1, lr=9.09e-03]\u001b[A\n",
            "Training:  99%|█████████▉| 1980/2000 [12:14<00:07,  2.83it/s, loss=1.8095, acc=0.603, ppl=6.1, lr=9.07e-03]\u001b[A\n",
            "Training: 100%|█████████▉| 1990/2000 [12:17<00:03,  2.90it/s, loss=1.8095, acc=0.603, ppl=6.1, lr=9.07e-03]\u001b[A\n",
            "Training: 100%|█████████▉| 1990/2000 [12:17<00:03,  2.90it/s, loss=1.8058, acc=0.603, ppl=6.1, lr=9.06e-03]\u001b[A\n",
            "Training: 100%|██████████| 2000/2000 [12:21<00:00,  2.70it/s, loss=1.8058, acc=0.603, ppl=6.1, lr=9.06e-03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ⏱️ Training completed in 741.2 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  📊 Final - Loss: 1.1151, Acc: 0.7511, PPL: 3.05\n",
            "💾 Saved final model to final_model.pt\n",
            "\n",
            "🎉 TRAINING COMPLETED!\n",
            "⏱️ Total time: 12.6 minutes\n",
            "🏆 Final Results:\n",
            "   Validation Loss: 1.1151\n",
            "   Validation Accuracy: 0.7511\n",
            "   Validation Perplexity: 3.05\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from transformers import AutoTokenizer\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"🌱 Set all seeds to {seed}\")\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    # Model architecture\n",
        "    d_model: int = 384\n",
        "    n_heads: int = 8\n",
        "    n_layers: int = 6\n",
        "    d_ff: int = 1536\n",
        "    batch_size: int = 24\n",
        "    max_steps: int = 2000\n",
        "\n",
        "    # Qwen3-like parameters\n",
        "    n_kv_heads: int = 4  # For Grouped-Query Attention\n",
        "    sliding_window: int = 4096  # Set a large default, effectively disabling it unless specified\n",
        "    attention_bias: bool = False  # Qwen3 often sets this to False\n",
        "    rms_norm_eps: float = 1e-6  # Epsilon for RMSNorm\n",
        "\n",
        "    # Training parameters\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    muon_lr: float = 0.01\n",
        "\n",
        "    # Data parameters\n",
        "    max_seq_len: int = 512\n",
        "    num_documents: int = 2000\n",
        "    max_tokens: int = 500000\n",
        "\n",
        "    # Evaluation\n",
        "    eval_every: int = 500\n",
        "    eval_steps: int = 100\n",
        "\n",
        "    # Regularization\n",
        "    weight_decay: float = 0.1\n",
        "    dropout: float = 0.1\n",
        "    grad_clip: float = 1.0\n",
        "\n",
        "    # Technical\n",
        "    use_amp: bool = True\n",
        "    vocab_size: Optional[int] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_k = self.d_model // self.n_heads\n",
        "        assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "        assert self.n_heads % self.n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
        "        self.n_kv_groups = self.n_heads // self.n_kv_heads\n",
        "\n",
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep).\n",
        "    The hidden states go from (batch, num_key_value_heads, seqlen, head_dim)\n",
        "    to (batch, num_attention_heads, seqlen, head_dim)\n",
        "    \"\"\"\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
        "\n",
        "@torch.compile\n",
        "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int = 5) -> torch.Tensor:\n",
        "    \"\"\"Newton-Schulz iteration to compute the zeroth power / orthogonalization of G.\"\"\"\n",
        "    assert G.ndim >= 2\n",
        "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
        "    X = G.bfloat16()\n",
        "\n",
        "    if G.size(-2) > G.size(-1):\n",
        "        X = X.mT\n",
        "\n",
        "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
        "\n",
        "    for _ in range(steps):\n",
        "        A = X @ X.mT\n",
        "        B = b * A + c * A @ A\n",
        "        X = a * X + B @ X\n",
        "\n",
        "    if G.size(-2) > G.size(-1):\n",
        "        X = X.mT\n",
        "\n",
        "    return X\n",
        "\n",
        "class Muon(torch.optim.Optimizer):\n",
        "    \"\"\"Muon - MomentUm Orthogonalized by Newton-schulz\"\"\"\n",
        "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):\n",
        "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                g = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                if \"momentum_buffer\" not in state:\n",
        "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
        "\n",
        "                buf = state[\"momentum_buffer\"]\n",
        "                buf.lerp_(g, 1 - group[\"momentum\"])\n",
        "                g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n",
        "                g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
        "                p.add_(g.view_as(p), alpha=-group[\"lr\"] * max(1, p.size(-2) / p.size(-1))**0.5)\n",
        "\n",
        "def load_and_cache_data(config: ModelConfig, cache_dir: str = \"data_cache\"):\n",
        "    \"\"\"Load and cache tokenized data to avoid reprocessing\"\"\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    cache_file = f\"{cache_dir}/tokenized_data_{config.num_documents}_{config.max_tokens}.pkl\"\n",
        "\n",
        "    # Check if cached data exists\n",
        "    if os.path.exists(cache_file):\n",
        "        print(f\"📦 Loading cached data from {cache_file}\")\n",
        "        with open(cache_file, 'rb') as f:\n",
        "            cached_data = pickle.load(f)\n",
        "\n",
        "        texts = cached_data['texts']\n",
        "        tokenizer = cached_data['tokenizer']\n",
        "        tokens = cached_data['tokens']\n",
        "        config.vocab_size = tokenizer.vocab_size\n",
        "\n",
        "        print(f\"✅ Loaded {len(texts)} documents, {len(tokens):,} tokens from cache\")\n",
        "        return texts, tokenizer, tokens\n",
        "\n",
        "    print(f\"🔄 Processing new data (will cache for future use)\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\", token=False)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", split=\"train\", streaming=True, token=False)\n",
        "\n",
        "    texts = []\n",
        "    for i, item in enumerate(dataset):\n",
        "        if i >= config.num_documents:\n",
        "            break\n",
        "        texts.append(item[\"text\"][:3000])\n",
        "\n",
        "    print(f\"Loaded {len(texts)} documents\")\n",
        "\n",
        "    # Tokenize\n",
        "    print(\"Tokenizing texts...\")\n",
        "    all_tokens = []\n",
        "    for text in tqdm(texts, desc=\"Tokenizing\"):\n",
        "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "    tokens = all_tokens[:config.max_tokens]\n",
        "    print(f\"Using {len(tokens):,} tokens\")\n",
        "    config.vocab_size = tokenizer.vocab_size\n",
        "\n",
        "    # Cache the processed data\n",
        "    cached_data = {'texts': texts, 'tokenizer': tokenizer, 'tokens': tokens}\n",
        "    with open(cache_file, 'wb') as f:\n",
        "        pickle.dump(cached_data, f)\n",
        "\n",
        "    print(f\"💾 Cached data to {cache_file}\")\n",
        "    return texts, tokenizer, tokens\n",
        "\n",
        "class TextTokenDataset(Dataset):\n",
        "    def __init__(self, tokens: List[int], seq_len: int = 512):\n",
        "        self.tokens = tokens\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, len(self.tokens) - self.seq_len)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.tokens[idx:idx + self.seq_len], dtype=torch.long)\n",
        "        y = torch.tensor(self.tokens[idx + 1:idx + self.seq_len + 1], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "class Rotary(nn.Module):\n",
        "    def __init__(self, dim: int, max_seq_len: int):\n",
        "        super().__init__()\n",
        "        angular_freq = (1 / 10000) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)\n",
        "        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])\n",
        "        t = torch.arange(max_seq_len, dtype=torch.float32)\n",
        "        theta = torch.einsum(\"i,j -> ij\", t, angular_freq)\n",
        "        self.register_buffer('cos', theta.cos(), persistent=False)\n",
        "        self.register_buffer('sin', theta.sin(), persistent=False)\n",
        "\n",
        "    def forward(self, x_BTHD: torch.Tensor):\n",
        "        assert self.cos.size(0) >= x_BTHD.size(-3)\n",
        "        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]\n",
        "        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)\n",
        "        y1 = x1 * cos + x2 * sin\n",
        "        y2 = x1 * (-sin) + x2 * cos\n",
        "        return torch.cat((y1, y2), 3).type_as(x_BTHD)\n",
        "\n",
        "class Qwen3Attention(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.d_model = config.d_model\n",
        "        self.n_heads = config.n_heads\n",
        "        self.n_kv_heads = config.n_kv_heads\n",
        "        self.n_kv_groups = config.n_kv_groups\n",
        "        self.d_k = config.d_k\n",
        "\n",
        "        # Separate linear layers for Q, K, V\n",
        "        self.q_proj = nn.Linear(self.d_model, self.n_heads * self.d_k, bias=config.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.d_model, self.n_kv_heads * self.d_k, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.d_model, self.n_kv_heads * self.d_k, bias=config.attention_bias)\n",
        "        self.w_o = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "\n",
        "        # QK-Normalization layers\n",
        "        self.q_norm = nn.RMSNorm(self.d_k, eps=config.rms_norm_eps)\n",
        "        self.k_norm = nn.RMSNorm(self.d_k, eps=config.rms_norm_eps)\n",
        "\n",
        "        self.rotary = Rotary(self.d_k, config.max_seq_len)\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "\n",
        "        # 1. Project Q, K, V separately\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "\n",
        "        # 2. Reshape into heads\n",
        "        q = q.view(batch_size, seq_len, self.n_heads, self.d_k)\n",
        "        k = k.view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n",
        "        v = v.view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n",
        "\n",
        "        # 3. Apply QK-Norm\n",
        "        q = self.q_norm(q)\n",
        "        k = self.k_norm(k)\n",
        "\n",
        "        # 4. Apply RoPE\n",
        "        # Transpose to (batch, seq_len, n_heads, d_k) -> (batch, n_heads, seq_len, d_k) for rotary\n",
        "        q = self.rotary(q.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
        "        k = self.rotary(k.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Transpose for attention: (batch, seq_len, n_heads, d_k) -> (batch, n_heads, seq_len, d_k)\n",
        "        Q = q.transpose(1, 2)\n",
        "        K = k.transpose(1, 2)\n",
        "        V = v.transpose(1, 2)\n",
        "\n",
        "        # 5. Repeat K and V heads for GQA\n",
        "        K = repeat_kv(K, self.n_kv_groups)\n",
        "        V = repeat_kv(V, self.n_kv_groups)\n",
        "\n",
        "        # 6. Scaled Dot-Product Attention\n",
        "        attn_output = F.scaled_dot_product_attention(\n",
        "            Q, K, V, is_causal=True, dropout_p=self.dropout if self.training else 0.0\n",
        "        )\n",
        "\n",
        "        # 7. Reshape and final projection\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        return self.w_o(attn_output)\n",
        "\n",
        "class SwiGLUFeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.gate_proj = nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.down_proj = nn.Linear(d_ff, d_model, bias=False)\n",
        "        self.up_proj = nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Implementation of the SwiGLU activation function\n",
        "        # F.silu is the Swish activation function\n",
        "        activated_x = F.silu(self.gate_proj(x)) * self.up_proj(x)\n",
        "        return self.down_proj(self.dropout(activated_x))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):  # Pass the entire config object\n",
        "        super().__init__()\n",
        "        self.attention = Qwen3Attention(config)\n",
        "        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n",
        "        self.norm1 = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
        "        self.norm2 = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out = self.attention(self.norm1(x))\n",
        "        x = x + self.dropout(attn_out)\n",
        "        ff_out = self.feed_forward(self.norm2(x))\n",
        "        x = x + self.dropout(ff_out)\n",
        "        return x\n",
        "\n",
        "class MinimalLLM(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.position_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(config) for _ in range(config.n_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
        "        self.output_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Tie weights\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.token_embedding.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_embedding(x) * math.sqrt(self.config.d_model)\n",
        "        x = self.position_dropout(x)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.output_dropout(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "def evaluate_model(model: nn.Module, val_loader: DataLoader, config: ModelConfig):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (x, y) in enumerate(val_loader):\n",
        "            if i >= config.eval_steps:\n",
        "                break\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            with autocast(enabled=config.use_amp):\n",
        "                logits = model(x)\n",
        "                loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "\n",
        "            total_loss += loss.item() * y.numel()\n",
        "            total_tokens += y.numel()\n",
        "\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "            total_correct += (predictions == y).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    accuracy = total_correct / total_tokens\n",
        "    perplexity = math.exp(min(avg_loss, 20))\n",
        "\n",
        "    model.train()\n",
        "    return {'val_loss': avg_loss, 'val_accuracy': accuracy, 'val_perplexity': perplexity}\n",
        "\n",
        "def setup_muon_optimizer(model: nn.Module, config: ModelConfig):\n",
        "    \"\"\"Setup Muon optimizer with hybrid approach\"\"\"\n",
        "    muon_params = []\n",
        "    adamw_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if (param.ndim == 2 and\n",
        "            'token_embedding' not in name and\n",
        "            'norm' not in name and\n",
        "            param.requires_grad):\n",
        "            muon_params.append(param)\n",
        "        else:\n",
        "            adamw_params.append(param)\n",
        "\n",
        "    print(f\"  Muon parameters: {sum(p.numel() for p in muon_params):,}\")\n",
        "    print(f\"  AdamW parameters: {sum(p.numel() for p in adamw_params):,}\")\n",
        "\n",
        "    muon_optimizer = Muon(muon_params, lr=config.muon_lr, momentum=0.95)\n",
        "    adamw_optimizer = torch.optim.AdamW(adamw_params, lr=config.muon_lr*0.1, weight_decay=config.weight_decay)\n",
        "\n",
        "    return [muon_optimizer, adamw_optimizer]\n",
        "\n",
        "def train_model(config: ModelConfig, train_loader: DataLoader, val_loader: DataLoader):\n",
        "    \"\"\"Train the model with Muon optimizer\"\"\"\n",
        "    print(f\"\\n🚀 Training Small model with Muon optimizer\")\n",
        "\n",
        "    # Initialize model\n",
        "    set_seed(42)\n",
        "    model = MinimalLLM(config)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"  📊 Total parameters: {total_params:,}\")\n",
        "\n",
        "    # Setup optimizers\n",
        "    optimizers = setup_muon_optimizer(model, config)\n",
        "\n",
        "    # Learning rate schedule\n",
        "    schedulers = []\n",
        "    for optimizer in optimizers:\n",
        "        warmup_steps = config.max_steps // 20\n",
        "        def lr_lambda(step):\n",
        "            if step < warmup_steps:\n",
        "                return step / warmup_steps\n",
        "            else:\n",
        "                progress = (step - warmup_steps) / (config.max_steps - warmup_steps)\n",
        "                return 0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "        schedulers.append(scheduler)\n",
        "\n",
        "    scaler = GradScaler() if config.use_amp else None\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    step = 0\n",
        "    start_time = time.time()\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    pbar = tqdm(total=config.max_steps, desc=\"Training\")\n",
        "\n",
        "    while step < config.max_steps:\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            if step >= config.max_steps:\n",
        "                break\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Forward pass with gradient accumulation\n",
        "            if config.use_amp:\n",
        "                with autocast():\n",
        "                    logits = model(x)\n",
        "                    loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "                    loss = loss / config.gradient_accumulation_steps\n",
        "                scaler.scale(loss).backward()\n",
        "            else:\n",
        "                logits = model(x)\n",
        "                loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
        "                loss = loss / config.gradient_accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "            # Optimizer step after accumulation\n",
        "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "                if config.use_amp:\n",
        "                    for optimizer in optimizers:\n",
        "                        scaler.unscale_(optimizer)\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "\n",
        "                    for optimizer in optimizers:\n",
        "                        scaler.step(optimizer)\n",
        "                        optimizer.zero_grad()\n",
        "                    for scheduler in schedulers:\n",
        "                        scheduler.step()\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "                    for optimizer in optimizers:\n",
        "                        optimizer.step()\n",
        "                        optimizer.zero_grad()\n",
        "                    for scheduler in schedulers:\n",
        "                        scheduler.step()\n",
        "\n",
        "            # Logging\n",
        "            if step % 10 == 0:\n",
        "                with torch.no_grad():\n",
        "                    predictions = logits.argmax(dim=-1)\n",
        "                    accuracy = (predictions == y).float().mean().item()\n",
        "                    current_loss = loss.item() * config.gradient_accumulation_steps\n",
        "                    perplexity = math.exp(min(current_loss, 20))\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{current_loss:.4f}',\n",
        "                    'acc': f'{accuracy:.3f}',\n",
        "                    'ppl': f'{perplexity:.1f}',\n",
        "                    'lr': f'{optimizers[0].param_groups[0][\"lr\"]:.2e}'\n",
        "                })\n",
        "\n",
        "            # Evaluation\n",
        "            if step % config.eval_every == 0 and step > 0:\n",
        "                eval_metrics = evaluate_model(model, val_loader, config)\n",
        "                print(f\"\\nStep {step}: Val Loss: {eval_metrics['val_loss']:.4f}, \"\n",
        "                      f\"Val Acc: {eval_metrics['val_accuracy']:.4f}, \"\n",
        "                      f\"Val PPL: {eval_metrics['val_perplexity']:.2f}\")\n",
        "\n",
        "                if eval_metrics['val_loss'] < best_val_loss:\n",
        "                    best_val_loss = eval_metrics['val_loss']\n",
        "                    # Save best model\n",
        "                    torch.save({\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'config': config,\n",
        "                        'step': step,\n",
        "                        'best_val_loss': best_val_loss,\n",
        "                        'final_metrics': eval_metrics\n",
        "                    }, 'best_model.pt')\n",
        "                    print(f\"💾 Saved best model with val_loss: {best_val_loss:.4f}\")\n",
        "\n",
        "            step += 1\n",
        "            if step % 10 == 0:\n",
        "                pbar.update(10)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"  ⏱️ Training completed in {training_time:.1f} seconds\")\n",
        "\n",
        "    # Final evaluation\n",
        "    final_eval = evaluate_model(model, val_loader, config)\n",
        "    print(f\"  📊 Final - Loss: {final_eval['val_loss']:.4f}, \"\n",
        "          f\"Acc: {final_eval['val_accuracy']:.4f}, PPL: {final_eval['val_perplexity']:.2f}\")\n",
        "\n",
        "    # Save final model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'config': config,\n",
        "        'step': step,\n",
        "        'final_metrics': final_eval\n",
        "    }, 'final_model.pt')\n",
        "    print(f\"💾 Saved final model to final_model.pt\")\n",
        "\n",
        "    return model, final_eval\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check system\n",
        "    print(f\"🔍 Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(42)\n",
        "\n",
        "    # Create config for Small model\n",
        "    config = ModelConfig()\n",
        "    print(f\"\\n📋 Model Configuration:\")\n",
        "    print(f\"   Architecture: {config.d_model}d, {config.n_layers}L, {config.n_heads}H, {config.d_ff}ff\")\n",
        "    print(f\"   Training: {config.max_steps} steps, batch size {config.batch_size}\")\n",
        "    print(f\"   Data: {config.max_tokens:,} tokens, seq_len {config.max_seq_len}\")\n",
        "\n",
        "    # Load data\n",
        "    texts, tokenizer, tokens = load_and_cache_data(config)\n",
        "    dataset = TextTokenDataset(tokens, config.max_seq_len)\n",
        "\n",
        "    # Train/val split\n",
        "    val_size = len(dataset) // 10\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"📊 Dataset: {len(train_dataset)} train, {len(val_dataset)} val samples\")\n",
        "\n",
        "    # Train model\n",
        "    start_time = time.time()\n",
        "    model, final_metrics = train_model(config, train_loader, val_loader)\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\n🎉 TRAINING COMPLETED!\")\n",
        "    print(f\"⏱️ Total time: {total_time/60:.1f} minutes\")\n",
        "    print(f\"🏆 Final Results:\")\n",
        "    print(f\"   Validation Loss: {final_metrics['val_loss']:.4f}\")\n",
        "    print(f\"   Validation Accuracy: {final_metrics['val_accuracy']:.4f}\")\n",
        "    print(f\"   Validation Perplexity: {final_metrics['val_perplexity']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "CEww1UNGPWNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import torch.serialization\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import the model classes from the training file\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\" Set all seeds to {seed}\")\n",
        "\n",
        "class TextGenerator:\n",
        "    \"\"\"Text generation class for the trained model\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str = \"final_model.pt\", tokenizer_path: str = \"HuggingFaceTB/SmolLM-135M\", device: str = \"auto\"):\n",
        "        \"\"\"\n",
        "        Initialize the text generator\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to the saved model checkpoint (default: final_model.pt)\n",
        "            tokenizer_path: Path to the tokenizer (default uses the same as training)\n",
        "            device: Device to run inference on (\"auto\", \"cpu\", \"cuda\")\n",
        "        \"\"\"\n",
        "        self.device = self._get_device(device)\n",
        "        print(f\"🔧 Using device: {self.device}\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        print(\"📚 Loading tokenizer...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, token=False)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Load model\n",
        "        print(\"🤖 Loading model...\")\n",
        "        self.model, self.config = self._load_model(model_path)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        print(f\"✅ Model loaded successfully!\")\n",
        "        print(f\"   Parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "        print(f\"   Vocabulary size: {self.config.vocab_size}\")\n",
        "        print(f\"   Max sequence length: {self.config.max_seq_len}\")\n",
        "\n",
        "    def _get_device(self, device: str) -> torch.device:\n",
        "        \"\"\"Determine the best device to use\"\"\"\n",
        "        if device == \"auto\":\n",
        "            if torch.cuda.is_available():\n",
        "                return torch.device(\"cuda\")\n",
        "            else:\n",
        "                return torch.device(\"cpu\")\n",
        "        else:\n",
        "            return torch.device(device)\n",
        "\n",
        "    def _load_model(self, model_path: str) -> Tuple[MinimalLLM, ModelConfig]:\n",
        "        \"\"\"Load the trained model from checkpoint\"\"\"\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Model checkpoint not found: {model_path}\")\n",
        "\n",
        "        # Add ModelConfig to safe globals for PyTorch 2.6+ compatibility\n",
        "        torch.serialization.add_safe_globals([ModelConfig])\n",
        "\n",
        "        try:\n",
        "            # Try loading with weights_only=True first (PyTorch 2.6+ default)\n",
        "            checkpoint = torch.load(model_path, map_location=self.device, weights_only=True)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  weights_only=True failed, trying weights_only=False: {e}\")\n",
        "            # Fallback to weights_only=False for older checkpoints\n",
        "            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)\n",
        "\n",
        "        # Extract config and create model\n",
        "        config = checkpoint['config']\n",
        "        model = MinimalLLM(config)\n",
        "\n",
        "        # Load state dict\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        return model, config\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 100,\n",
        "        temperature: float = 0.8,\n",
        "        top_p: float = 0.9,\n",
        "        top_k: int = 50,\n",
        "        do_sample: bool = True,\n",
        "        num_return_sequences: int = 1,\n",
        "        stop_tokens: Optional[List[str]] = None\n",
        "    ) -> List[str]:\n",
        "        \"\"\"\n",
        "        Generate text from a prompt\n",
        "\n",
        "        Args:\n",
        "            prompt: Input text prompt\n",
        "            max_length: Maximum length of generated text (including prompt)\n",
        "            temperature: Sampling temperature (higher = more random)\n",
        "            top_p: Nucleus sampling parameter\n",
        "            top_k: Top-k sampling parameter\n",
        "            do_sample: Whether to use sampling (False for greedy decoding)\n",
        "            num_return_sequences: Number of sequences to generate\n",
        "            stop_tokens: List of tokens to stop generation at\n",
        "\n",
        "        Returns:\n",
        "            List of generated text sequences\n",
        "        \"\"\"\n",
        "        # Tokenize prompt\n",
        "        input_ids = self.tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "        input_ids = input_ids.to(self.device)\n",
        "\n",
        "        # Convert stop tokens to IDs\n",
        "        stop_token_ids = []\n",
        "        if stop_tokens:\n",
        "            for token in stop_tokens:\n",
        "                token_id = self.tokenizer.encode(token, add_special_tokens=False)\n",
        "                if token_id:\n",
        "                    stop_token_ids.extend(token_id)\n",
        "\n",
        "        generated_sequences = []\n",
        "\n",
        "        for _ in range(num_return_sequences):\n",
        "            # Generate sequence\n",
        "            generated_ids = self._generate_sequence(\n",
        "                input_ids=input_ids,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                top_k=top_k,\n",
        "                do_sample=do_sample,\n",
        "                stop_token_ids=stop_token_ids\n",
        "            )\n",
        "\n",
        "            # Decode to text\n",
        "            generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "            generated_sequences.append(generated_text)\n",
        "\n",
        "        return generated_sequences\n",
        "\n",
        "    def _generate_sequence(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        max_length: int,\n",
        "        temperature: float,\n",
        "        top_p: float,\n",
        "        top_k: int,\n",
        "        do_sample: bool,\n",
        "        stop_token_ids: List[int]\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Generate a single sequence using the model\"\"\"\n",
        "\n",
        "        current_ids = input_ids.clone()\n",
        "        generated_length = current_ids.shape[1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            while generated_length < max_length:\n",
        "                # Get model predictions\n",
        "                logits = self.model(current_ids)\n",
        "                next_token_logits = logits[0, -1, :] / temperature\n",
        "\n",
        "                # Apply top-k filtering\n",
        "                if top_k > 0:\n",
        "                    top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
        "                    next_token_logits = torch.full_like(next_token_logits, float('-inf'))\n",
        "                    next_token_logits[top_k_indices] = top_k_logits\n",
        "\n",
        "                # Apply top-p (nucleus) filtering\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                    # Remove tokens with cumulative probability above the threshold\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
        "                    sorted_indices_to_remove[0] = 0\n",
        "\n",
        "                    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                    next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "                # Sample or take argmax\n",
        "                if do_sample:\n",
        "                    probs = F.softmax(next_token_logits, dim=-1)\n",
        "                    next_token = torch.multinomial(probs, num_samples=1)\n",
        "                else:\n",
        "                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "                # Ensure next_token has the correct shape for concatenation\n",
        "                # next_token should be (1, 1) to match current_ids shape (1, seq_len)\n",
        "                if next_token.dim() == 1:\n",
        "                    next_token = next_token.unsqueeze(0)\n",
        "\n",
        "                # Check for stop tokens\n",
        "                if next_token.item() in stop_token_ids:\n",
        "                    break\n",
        "\n",
        "                # Append to sequence\n",
        "                current_ids = torch.cat([current_ids, next_token], dim=1)\n",
        "                generated_length += 1\n",
        "\n",
        "        return current_ids[0]\n",
        "\n",
        "    def get_perplexity(self, text: str) -> float:\n",
        "        \"\"\"Calculate perplexity of the given text\"\"\"\n",
        "        # Tokenize text\n",
        "        tokens = self.tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "        if len(tokens) < 2:\n",
        "            return float('inf')\n",
        "\n",
        "        # Create sequences for evaluation\n",
        "        sequences = []\n",
        "        for i in range(len(tokens) - 1):\n",
        "            sequences.append((tokens[i], tokens[i + 1]))\n",
        "\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for input_token, target_token in sequences:\n",
        "                # Create input tensor\n",
        "                input_tensor = torch.tensor([[input_token]], device=self.device)\n",
        "                target_tensor = torch.tensor([[target_token]], device=self.device)\n",
        "\n",
        "                # Get model prediction\n",
        "                logits = self.model(input_tensor)\n",
        "                loss = F.cross_entropy(logits.view(-1, self.config.vocab_size), target_tensor.view(-1))\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                total_tokens += 1\n",
        "\n",
        "        avg_loss = total_loss / total_tokens\n",
        "        perplexity = math.exp(avg_loss)\n",
        "\n",
        "        return perplexity\n",
        "\n",
        "def interactive_mode(generator: TextGenerator):\n",
        "    \"\"\"Run interactive text generation mode\"\"\"\n",
        "    print(\"\\n🎭 Interactive Text Generation Mode\")\n",
        "    print(\"Type 'quit' to exit, 'help' for commands\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            prompt = input(\"\\n Enter your prompt: \").strip()\n",
        "\n",
        "            if prompt.lower() == 'quit':\n",
        "                print(\"👋 Goodbye!\")\n",
        "                break\n",
        "            elif prompt.lower() == 'help':\n",
        "                print(\"\\n Available commands:\")\n",
        "                print(\"  help - Show this help message\")\n",
        "                print(\"  quit - Exit the program\")\n",
        "                print(\"  settings - Show current generation settings\")\n",
        "                print(\"  sample - Generate with sampling\")\n",
        "                print(\"  greedy - Generate with greedy decoding\")\n",
        "                print(\"\\n💡 Tips:\")\n",
        "                print(\"  - Use 'sample' or 'greedy' prefix to change generation mode\")\n",
        "                print(\"  - Example: 'sample The quick brown fox'\")\n",
        "                continue\n",
        "            elif prompt.lower() == 'settings':\n",
        "                print(\"\\n⚙️ Current settings:\")\n",
        "                print(\"  Temperature: 0.8\")\n",
        "                print(\"  Top-p: 0.9\")\n",
        "                print(\"  Top-k: 50\")\n",
        "                print(\"  Max length: 100\")\n",
        "                continue\n",
        "\n",
        "            # Check for mode prefixes\n",
        "            do_sample = True\n",
        "            if prompt.startswith('sample '):\n",
        "                prompt = prompt[7:]\n",
        "                do_sample = True\n",
        "            elif prompt.startswith('greedy '):\n",
        "                prompt = prompt[7:]\n",
        "                do_sample = False\n",
        "\n",
        "            if not prompt:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n Generating text...\")\n",
        "            generated_texts = generator.generate(\n",
        "                prompt=prompt,\n",
        "                max_length=100,\n",
        "                temperature=0.8,\n",
        "                top_p=0.9,\n",
        "                top_k=50,\n",
        "                do_sample=do_sample,\n",
        "                num_return_sequences=1\n",
        "            )\n",
        "\n",
        "            print(f\"\\n✨ Generated text:\")\n",
        "            print(\"-\" * 40)\n",
        "            for i, text in enumerate(generated_texts, 1):\n",
        "                print(f\"{i}. {text}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n👋 Goodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {e}\")\n",
        "\n",
        "def main():\n",
        "    # Set seed\n",
        "    set_seed(42)\n",
        "\n",
        "    # Initialize generator with default settings\n",
        "    try:\n",
        "        generator = TextGenerator(\"final_model.pt\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to load model: {e}\")\n",
        "        return\n",
        "\n",
        "    # Run interactive mode\n",
        "    interactive_mode(generator)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jroez0sMLd4f",
        "outputId": "f0e26d54-b766-45d4-c70d-ac940abcadb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Set all seeds to 42\n",
            "🔧 Using device: cuda\n",
            "📚 Loading tokenizer...\n",
            "🤖 Loading model...\n",
            "✅ Model loaded successfully!\n",
            "   Parameters: 32,150,976\n",
            "   Vocabulary size: 49152\n",
            "   Max sequence length: 512\n",
            "\n",
            "🎭 Interactive Text Generation Mode\n",
            "Type 'quit' to exit, 'help' for commands\n",
            "==================================================\n",
            "\n",
            " Enter your prompt: the future of AI is\n",
            "\n",
            " Generating text...\n",
            "\n",
            "✨ Generated text:\n",
            "----------------------------------------\n",
            "1. the future of AI is like the First, the way, the right away to build something called an online, and take care of like a lot of this, it.\n",
            "\n",
            "So why is it important? Well, understanding and keep track of this important to keep track of like exploring and understand market trends and why using smart decisions. By fostering a better future. Who knows, maybe someday you'll become an example of this incredible inventions or problems in helping us. Happy exploring! Chapter 10\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4MaArKGQ-xI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}